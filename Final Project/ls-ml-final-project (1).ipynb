{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install torch transformers datasets matplotlib pandas tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T21:06:29.793014Z","iopub.execute_input":"2025-07-08T21:06:29.793187Z","iopub.status.idle":"2025-07-08T21:07:51.905139Z","shell.execute_reply.started":"2025-07-08T21:06:29.793170Z","shell.execute_reply":"2025-07-08T21:07:51.904073Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec (from torch)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m106.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.5.1\n    Uninstalling fsspec-2025.5.1:\n      Successfully uninstalled fsspec-2025.5.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2025.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# from torch.utils.data import Dataset, DataLoader\n# from transformers import (\n#     GPT2LMHeadModel, \n#     GPT2Tokenizer, \n#     TrainingArguments, \n#     Trainer,\n#     DataCollatorForLanguageModeling\n# )\n# from datasets import load_dataset\n# import numpy as np\n# from tqdm import tqdm\n# import matplotlib.pyplot as plt\n# import pandas as pd\n# from typing import Dict, List, Tuple\n# import math\n\n# class WikiTextDataset(Dataset):\n#     \"\"\"Custom dataset for WikiText-2 with proper tokenization\"\"\"\n    \n#     def __init__(self, texts: List[str], tokenizer, max_length: int = 512):\n#         self.tokenizer = tokenizer\n#         self.max_length = max_length\n#         self.examples = []\n        \n#         print(\"Processing texts...\")\n#         for text in tqdm(texts):\n#             if len(text.strip()) > 0:  # Skip empty texts\n#                 # Tokenize and create sliding windows\n#                 tokens = tokenizer.encode(text, add_special_tokens=True)\n                \n#                 # Create overlapping sequences\n#                 for i in range(0, len(tokens) - 1, max_length // 2):\n#                     chunk = tokens[i:i + max_length]\n#                     if len(chunk) > 1:  # Need at least 2 tokens (input + target)\n#                         self.examples.append(chunk)\n    \n#     def __len__(self):\n#         return len(self.examples)\n    \n#     def __getitem__(self, idx):\n#         return {\n#             'input_ids': torch.tensor(self.examples[idx], dtype=torch.long),\n#             'attention_mask': torch.ones(len(self.examples[idx]), dtype=torch.long)\n#         }\n\n# class NWPEvaluator:\n#     \"\"\"Evaluator for Next Word Prediction metrics\"\"\"\n    \n#     def __init__(self, model, tokenizer, device):\n#         self.model = model\n#         self.tokenizer = tokenizer\n#         self.device = device\n#         self.model.eval()\n    \n#     def calculate_perplexity(self, dataloader) -> float:\n#         \"\"\"Calculate perplexity on evaluation dataset\"\"\"\n#         total_loss = 0\n#         total_tokens = 0\n        \n#         with torch.no_grad():\n#             for batch in tqdm(dataloader, desc=\"Calculating perplexity\"):\n#                 input_ids = batch['input_ids'].to(self.device)\n#                 attention_mask = batch['attention_mask'].to(self.device)\n                \n#                 # Shift labels for next token prediction\n#                 labels = input_ids.clone()\n#                 labels[:, :-1] = input_ids[:, 1:]\n#                 labels[:, -1] = -100  # Ignore last token\n                \n#                 outputs = self.model(\n#                     input_ids=input_ids,\n#                     attention_mask=attention_mask,\n#                     labels=labels\n#                 )\n                \n#                 loss = outputs.loss\n#                 # Count valid tokens (not -100)\n#                 valid_tokens = (labels != -100).sum().item()\n                \n#                 total_loss += loss.item() * valid_tokens\n#                 total_tokens += valid_tokens\n        \n#         avg_loss = total_loss / total_tokens\n#         perplexity = math.exp(avg_loss)\n#         return perplexity\n    \n#     def calculate_top_k_accuracy(self, dataloader, k_values: List[int] = [1, 5, 10]) -> Dict[int, float]:\n#         \"\"\"Calculate top-k accuracy for next word prediction\"\"\"\n#         correct_predictions = {k: 0 for k in k_values}\n#         total_predictions = 0\n        \n#         with torch.no_grad():\n#             for batch in tqdm(dataloader, desc=\"Calculating top-k accuracy\"):\n#                 input_ids = batch['input_ids'].to(self.device)\n#                 attention_mask = batch['attention_mask'].to(self.device)\n                \n#                 for seq_idx in range(input_ids.size(0)):\n#                     sequence = input_ids[seq_idx]\n#                     mask = attention_mask[seq_idx]\n                    \n#                     # Get valid length\n#                     valid_length = mask.sum().item()\n                    \n#                     # Predict each position (except last)\n#                     for pos in range(1, valid_length):\n#                         context = sequence[:pos].unsqueeze(0)\n#                         target = sequence[pos].item()\n                        \n#                         outputs = self.model(context)\n#                         logits = outputs.logits[0, -1, :]  # Last position logits\n                        \n#                         # Get top-k predictions\n#                         top_k_max = max(k_values)\n#                         top_k_tokens = torch.topk(logits, top_k_max).indices\n                        \n#                         # Check accuracy for different k values\n#                         for k in k_values:\n#                             if target in top_k_tokens[:k]:\n#                                 correct_predictions[k] += 1\n                        \n#                         total_predictions += 1\n        \n#         # Calculate accuracies\n#         accuracies = {k: correct_predictions[k] / total_predictions for k in k_values}\n#         return accuracies\n    \n#     def sample_predictions(self, text: str, num_predictions: int = 5) -> List[str]:\n#         \"\"\"Generate sample predictions for demonstration\"\"\"\n#         tokens = self.tokenizer.encode(text, return_tensors='pt').to(self.device)\n        \n#         predictions = []\n#         with torch.no_grad():\n#             for _ in range(num_predictions):\n#                 outputs = self.model(tokens)\n#                 logits = outputs.logits[0, -1, :]\n                \n#                 # Sample from distribution\n#                 probs = torch.softmax(logits, dim=-1)\n#                 next_token = torch.multinomial(probs, 1)\n                \n#                 predicted_word = self.tokenizer.decode(next_token.item())\n#                 predictions.append(predicted_word)\n                \n#                 # Add to context for next prediction\n#                 tokens = torch.cat([tokens, next_token.unsqueeze(0)], dim=1)\n        \n#         return predictions\n\n# def load_and_prepare_data():\n#     \"\"\"Load WikiText-2 dataset and prepare for training\"\"\"\n#     print(\"Loading WikiText-2 dataset...\")\n#     dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n    \n#     # Filter out empty texts\n#     train_texts = [text for text in dataset['train']['text'] if len(text.strip()) > 0]\n#     valid_texts = [text for text in dataset['validation']['text'] if len(text.strip()) > 0]\n#     test_texts = [text for text in dataset['test']['text'] if len(text.strip()) > 0]\n    \n#     print(f\"Train texts: {len(train_texts)}\")\n#     print(f\"Validation texts: {len(valid_texts)}\")\n#     print(f\"Test texts: {len(test_texts)}\")\n    \n#     return train_texts, valid_texts, test_texts\n\n# def setup_model_and_tokenizer():\n#     \"\"\"Initialize GPT-2 model and tokenizer\"\"\"\n#     print(\"Setting up GPT-2 model and tokenizer...\")\n#     model_name = \"gpt2\"\n    \n#     tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n#     model = GPT2LMHeadModel.from_pretrained(model_name)\n    \n#     # Add padding token\n#     tokenizer.pad_token = tokenizer.eos_token\n    \n#     return model, tokenizer\n\n# def fine_tune_model(model, tokenizer, train_dataset, eval_dataset, output_dir=\"./gpt2-wikitext2\"):\n#     \"\"\"Fine-tune GPT-2 on WikiText-2\"\"\"\n    \n#     # Data collator for language modeling\n#     data_collator = DataCollatorForLanguageModeling(\n#         tokenizer=tokenizer,\n#         mlm=False,  # GPT-2 is autoregressive, not masked LM\n#         pad_to_multiple_of=8,\n#         return_tensors=\"pt\"\n#     )\n    \n#     # Training arguments\n#     training_args = TrainingArguments(\n#         output_dir=output_dir,\n#         overwrite_output_dir=True,\n#         num_train_epochs=3,\n#         per_device_train_batch_size=4,\n#         per_device_eval_batch_size=4,\n#         warmup_steps=500,\n#         logging_steps=100,\n#         save_steps=1000,\n#         eval_steps=500,\n#         eval_strategy=\"steps\",  # Changed from evaluation_strategy\n#         load_best_model_at_end=True,\n#         save_total_limit=2,\n#         gradient_accumulation_steps=2,\n#         fp16=True,  # Mixed precision training\n#         dataloader_pin_memory=True,\n#         learning_rate=5e-5,\n#         weight_decay=0.01,\n#         adam_epsilon=1e-8,\n#         max_grad_norm=1.0,\n#         lr_scheduler_type=\"linear\",\n#         report_to=[],  # Changed from None to empty list\n#     )\n    \n#     # Initialize trainer\n#     trainer = Trainer(\n#         model=model,\n#         args=training_args,\n#         data_collator=data_collator,\n#         train_dataset=train_dataset,\n#         eval_dataset=eval_dataset,\n#         tokenizer=tokenizer,\n#     )\n    \n#     # Train the model\n#     print(\"Starting fine-tuning...\")\n#     trainer.train()\n    \n#     # Save the fine-tuned model\n#     trainer.save_model()\n#     tokenizer.save_pretrained(output_dir)\n    \n#     return trainer\n\n# def evaluate_model(model, tokenizer, test_dataset, device):\n#     \"\"\"Comprehensive evaluation of the fine-tuned model\"\"\"\n#     print(\"Evaluating fine-tuned model...\")\n    \n#     # Create test dataloader\n#     test_dataloader = DataLoader(\n#         test_dataset, \n#         batch_size=8, \n#         shuffle=False,\n#         collate_fn=lambda x: {\n#             'input_ids': torch.nn.utils.rnn.pad_sequence(\n#                 [item['input_ids'] for item in x], \n#                 batch_first=True, \n#                 padding_value=tokenizer.pad_token_id\n#             ),\n#             'attention_mask': torch.nn.utils.rnn.pad_sequence(\n#                 [item['attention_mask'] for item in x], \n#                 batch_first=True, \n#                 padding_value=0\n#             )\n#         }\n#     )\n    \n#     # Initialize evaluator\n#     evaluator = NWPEvaluator(model, tokenizer, device)\n    \n#     # Calculate perplexity\n#     perplexity = evaluator.calculate_perplexity(test_dataloader)\n#     print(f\"Perplexity: {perplexity:.2f}\")\n    \n#     # Calculate top-k accuracy\n#     accuracies = evaluator.calculate_top_k_accuracy(test_dataloader, k_values=[1, 5, 10])\n#     print(\"Top-k Accuracies:\")\n#     for k, acc in accuracies.items():\n#         print(f\"  Top-{k}: {acc:.4f}\")\n    \n#     return perplexity, accuracies, evaluator\n\n# def demonstrate_predictions(evaluator, tokenizer):\n#     \"\"\"Demonstrate model predictions on sample texts\"\"\"\n#     print(\"\\nSample Predictions:\")\n#     print(\"=\" * 50)\n    \n#     sample_texts = [\n#         \"Albert Einstein was a\",\n#         \"The capital of France is\",\n#         \"Machine learning is the\",\n#         \"In the year 2024, artificial intelligence\",\n#         \"The quick brown fox\"\n#     ]\n    \n#     for text in sample_texts:\n#         predictions = evaluator.sample_predictions(text, num_predictions=3)\n#         print(f\"Input: '{text}'\")\n#         print(f\"Predictions: {predictions}\")\n#         print(\"-\" * 30)\n\n# def main():\n#     # Set device\n#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#     print(f\"Using device: {device}\")\n    \n#     # Load data\n#     train_texts, valid_texts, test_texts = load_and_prepare_data()\n    \n#     # Setup model and tokenizer\n#     model, tokenizer = setup_model_and_tokenizer()\n#     model.to(device)\n    \n#     # Create datasets\n#     print(\"Creating datasets...\")\n#     train_dataset = WikiTextDataset(train_texts[:1000], tokenizer)  # Subset for demo\n#     eval_dataset = WikiTextDataset(valid_texts[:200], tokenizer)\n#     test_dataset = WikiTextDataset(test_texts[:200], tokenizer)\n    \n#     # Fine-tune model\n#     trainer = fine_tune_model(model, tokenizer, train_dataset, eval_dataset)\n    \n#     # Evaluate model\n#     perplexity, accuracies, evaluator = evaluate_model(model, tokenizer, test_dataset, device)\n    \n#     # Demonstrate predictions\n#     demonstrate_predictions(evaluator, tokenizer)\n    \n#     # Create results summary\n#     results = {\n#         'Metric': ['Perplexity', 'Top-1 Accuracy', 'Top-5 Accuracy', 'Top-10 Accuracy'],\n#         'Value': [f\"{perplexity:.2f}\", f\"{accuracies[1]:.4f}\", f\"{accuracies[5]:.4f}\", f\"{accuracies[10]:.4f}\"]\n#     }\n    \n#     results_df = pd.DataFrame(results)\n#     print(\"\\nFinal Results Summary:\")\n#     print(results_df.to_string(index=False))\n    \n#     return model, tokenizer, results_df\n\n# if __name__ == \"__main__\":\n#     # Note: This is a comprehensive implementation\n#     # For actual execution, you may want to run sections separately\n#     # due to computational requirements\n    \n#     print(\"GPT-2 Fine-tuning Pipeline for Next Word Prediction\")\n#     print(\"=\" * 60)\n    \n#     # Uncomment the following line to run the full pipeline\n#     # model, tokenizer, results = main()\n    \n#     print(\"Pipeline setup complete!\")\n#     print(\"To run: uncomment the main() call and execute with sufficient GPU memory\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T21:11:01.374991Z","iopub.execute_input":"2025-07-08T21:11:01.375308Z","iopub.status.idle":"2025-07-08T21:11:01.408686Z","shell.execute_reply.started":"2025-07-08T21:11:01.375285Z","shell.execute_reply":"2025-07-08T21:11:01.408032Z"}},"outputs":[{"name":"stdout","text":"GPT-2 Fine-tuning Pipeline for Next Word Prediction\n============================================================\nPipeline setup complete!\nTo run: uncomment the main() call and execute with sufficient GPU memory\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"model, tokenizer, results = main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T21:11:07.155778Z","iopub.execute_input":"2025-07-08T21:11:07.156460Z","iopub.status.idle":"2025-07-08T21:18:30.659049Z","shell.execute_reply.started":"2025-07-08T21:11:07.156427Z","shell.execute_reply":"2025-07-08T21:18:30.658343Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading WikiText-2 dataset...\nTrain texts: 23767\nValidation texts: 2461\nTest texts: 2891\nSetting up GPT-2 model and tokenizer...\nCreating datasets...\nProcessing texts...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1000/1000 [00:01<00:00, 991.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processing texts...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 200/200 [00:00<00:00, 1186.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processing texts...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 200/200 [00:00<00:00, 1082.17it/s]\n/tmp/ipykernel_36/4207624583.py:217: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"name":"stdout","text":"Starting fine-tuning...\n","output_type":"stream"},{"name":"stderr","text":"`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='204' max='204' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [204/204 02:03, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Evaluating fine-tuned model...\n","output_type":"stream"},{"name":"stderr","text":"Calculating perplexity: 100%|██████████| 27/27 [00:04<00:00,  6.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity: 17153.29\n","output_type":"stream"},{"name":"stderr","text":"Calculating top-k accuracy: 100%|██████████| 27/27 [05:05<00:00, 11.33s/it]\n","output_type":"stream"},{"name":"stdout","text":"Top-k Accuracies:\n  Top-1: 0.3728\n  Top-5: 0.5901\n  Top-10: 0.6654\n\nSample Predictions:\n==================================================\nInput: 'Albert Einstein was a'\nPredictions: [' analyst', ' for', ' the']\n------------------------------\nInput: 'The capital of France is'\nPredictions: [' covered', ' by', ' ceilings']\n------------------------------\nInput: 'Machine learning is the'\nPredictions: [' neural', ' process', ' that']\n------------------------------\nInput: 'In the year 2024, artificial intelligence'\nPredictions: [' developed', ' a', ' vehicle']\n------------------------------\nInput: 'The quick brown fox'\nPredictions: [' fired', ' off', ' a']\n------------------------------\n\nFinal Results Summary:\n         Metric    Value\n     Perplexity 17153.29\n Top-1 Accuracy   0.3728\n Top-5 Accuracy   0.5901\nTop-10 Accuracy   0.6654\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# from torch.utils.data import Dataset, DataLoader\n# from transformers import (\n#     GPT2LMHeadModel, \n#     GPT2Tokenizer, \n#     TrainingArguments, \n#     Trainer,\n#     DataCollatorForLanguageModeling\n# )\n# from datasets import load_dataset\n# import numpy as np\n# from tqdm import tqdm\n# import matplotlib.pyplot as plt\n# import pandas as pd\n# from typing import Dict, List, Tuple\n# import math\n\n# class WikiTextDataset(Dataset):\n#     \"\"\"Custom dataset for WikiText-2 with proper tokenization\"\"\"\n    \n#     def __init__(self, texts: List[str], tokenizer, max_length: int = 512):\n#         self.tokenizer = tokenizer\n#         self.max_length = max_length\n#         self.examples = []\n        \n#         print(\"Processing texts...\")\n#         for text in tqdm(texts):\n#             if len(text.strip()) > 0:  # Skip empty texts\n#                 # Tokenize and create sliding windows\n#                 tokens = tokenizer.encode(text, add_special_tokens=True)\n                \n#                 # Create overlapping sequences\n#                 for i in range(0, len(tokens) - 1, max_length // 2):\n#                     chunk = tokens[i:i + max_length]\n#                     if len(chunk) > 1:  # Need at least 2 tokens (input + target)\n#                         self.examples.append(chunk)\n    \n#     def __len__(self):\n#         return len(self.examples)\n    \n#     def __getitem__(self, idx):\n#         return {\n#             'input_ids': torch.tensor(self.examples[idx], dtype=torch.long),\n#             'attention_mask': torch.ones(len(self.examples[idx]), dtype=torch.long)\n#         }\n\n# class NWPEvaluator:\n#     \"\"\"Evaluator for Next Word Prediction metrics\"\"\"\n    \n#     def __init__(self, model, tokenizer, device):\n#         self.model = model\n#         self.tokenizer = tokenizer\n#         self.device = device\n#         self.model.eval()\n    \n#     def calculate_perplexity(self, dataloader) -> float:\n#         \"\"\"Calculate perplexity on evaluation dataset\"\"\"\n#         total_loss = 0\n#         total_tokens = 0\n        \n#         with torch.no_grad():\n#             for batch in tqdm(dataloader, desc=\"Calculating perplexity\"):\n#                 input_ids = batch['input_ids'].to(self.device)\n#                 attention_mask = batch['attention_mask'].to(self.device)\n                \n#                 # Shift labels for next token prediction\n#                 labels = input_ids.clone()\n#                 labels[:, :-1] = input_ids[:, 1:]\n#                 labels[:, -1] = -100  # Ignore last token\n                \n#                 outputs = self.model(\n#                     input_ids=input_ids,\n#                     attention_mask=attention_mask,\n#                     labels=labels\n#                 )\n                \n#                 loss = outputs.loss\n#                 # Count valid tokens (not -100)\n#                 valid_tokens = (labels != -100).sum().item()\n                \n#                 total_loss += loss.item() * valid_tokens\n#                 total_tokens += valid_tokens\n        \n#         avg_loss = total_loss / total_tokens\n#         perplexity = math.exp(avg_loss)\n#         return perplexity\n    \n#     def calculate_top_k_accuracy(self, dataloader, k_values: List[int] = [1, 5, 10]) -> Dict[int, float]:\n#         \"\"\"Calculate top-k accuracy for next word prediction\"\"\"\n#         correct_predictions = {k: 0 for k in k_values}\n#         total_predictions = 0\n        \n#         with torch.no_grad():\n#             for batch in tqdm(dataloader, desc=\"Calculating top-k accuracy\"):\n#                 input_ids = batch['input_ids'].to(self.device)\n#                 attention_mask = batch['attention_mask'].to(self.device)\n                \n#                 for seq_idx in range(input_ids.size(0)):\n#                     sequence = input_ids[seq_idx]\n#                     mask = attention_mask[seq_idx]\n                    \n#                     # Get valid length\n#                     valid_length = mask.sum().item()\n                    \n#                     # Predict each position (except last)\n#                     for pos in range(1, valid_length):\n#                         context = sequence[:pos].unsqueeze(0)\n#                         target = sequence[pos].item()\n                        \n#                         outputs = self.model(context)\n#                         logits = outputs.logits[0, -1, :]  # Last position logits\n                        \n#                         # Get top-k predictions\n#                         top_k_max = max(k_values)\n#                         top_k_tokens = torch.topk(logits, top_k_max).indices\n                        \n#                         # Check accuracy for different k values\n#                         for k in k_values:\n#                             if target in top_k_tokens[:k]:\n#                                 correct_predictions[k] += 1\n                        \n#                         total_predictions += 1\n        \n#         # Calculate accuracies\n#         accuracies = {k: correct_predictions[k] / total_predictions for k in k_values}\n#         return accuracies\n    \n#     def sample_predictions(self, text: str, num_predictions: int = 5) -> List[str]:\n#         \"\"\"Generate sample predictions for demonstration\"\"\"\n#         tokens = self.tokenizer.encode(text, return_tensors='pt').to(self.device)\n        \n#         predictions = []\n#         with torch.no_grad():\n#             for _ in range(num_predictions):\n#                 outputs = self.model(tokens)\n#                 logits = outputs.logits[0, -1, :]\n                \n#                 # Sample from distribution\n#                 probs = torch.softmax(logits, dim=-1)\n#                 next_token = torch.multinomial(probs, 1)\n                \n#                 predicted_word = self.tokenizer.decode(next_token.item())\n#                 predictions.append(predicted_word)\n                \n#                 # Add to context for next prediction\n#                 tokens = torch.cat([tokens, next_token.unsqueeze(0)], dim=1)\n        \n#         return predictions\n\n# def load_and_prepare_data():\n#     \"\"\"Load WikiText-2 dataset and prepare for training\"\"\"\n#     print(\"Loading WikiText-2 dataset...\")\n#     dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n    \n#     # Filter out empty texts\n#     train_texts = [text for text in dataset['train']['text'] if len(text.strip()) > 0]\n#     valid_texts = [text for text in dataset['validation']['text'] if len(text.strip()) > 0]\n#     test_texts = [text for text in dataset['test']['text'] if len(text.strip()) > 0]\n    \n#     print(f\"Train texts: {len(train_texts)}\")\n#     print(f\"Validation texts: {len(valid_texts)}\")\n#     print(f\"Test texts: {len(test_texts)}\")\n    \n#     return train_texts, valid_texts, test_texts\n\n# def setup_model_and_tokenizer():\n#     \"\"\"Initialize GPT-2 model and tokenizer\"\"\"\n#     print(\"Setting up GPT-2 model and tokenizer...\")\n#     model_name = \"gpt2\"\n    \n#     tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n#     model = GPT2LMHeadModel.from_pretrained(model_name)\n    \n#     # Add padding token\n#     tokenizer.pad_token = tokenizer.eos_token\n    \n#     return model, tokenizer\n\n# def fine_tune_model(model, tokenizer, train_dataset, eval_dataset, output_dir=\"./gpt2-wikitext2\"):\n#     \"\"\"Fine-tune GPT-2 on WikiText-2\"\"\"\n    \n#     # Data collator for language modeling\n#     data_collator = DataCollatorForLanguageModeling(\n#         tokenizer=tokenizer,\n#         mlm=False,  # GPT-2 is autoregressive, not masked LM\n#         pad_to_multiple_of=8,\n#         return_tensors=\"pt\"\n#     )\n    \n#     # Training arguments\n#     training_args = TrainingArguments(\n#         output_dir=output_dir,\n#         overwrite_output_dir=True,\n#         num_train_epochs=50,\n#         per_device_train_batch_size=4,\n#         per_device_eval_batch_size=4,\n#         warmup_steps=500,\n#         logging_steps=100,\n#         save_steps=1000,\n#         eval_steps=500,\n#         eval_strategy=\"steps\",  # Changed from evaluation_strategy\n#         load_best_model_at_end=True,\n#         save_total_limit=2,\n#         gradient_accumulation_steps=2,\n#         fp16=True,  # Mixed precision training\n#         dataloader_pin_memory=True,\n#         learning_rate=5e-5,\n#         weight_decay=0.01,\n#         adam_epsilon=1e-8,\n#         max_grad_norm=1.0,\n#         lr_scheduler_type=\"linear\",\n#         report_to=[],  # Changed from None to empty list\n#     )\n    \n#     # Initialize trainer\n#     trainer = Trainer(\n#         model=model,\n#         args=training_args,\n#         data_collator=data_collator,\n#         train_dataset=train_dataset,\n#         eval_dataset=eval_dataset,\n#         tokenizer=tokenizer,\n#     )\n    \n#     # Train the model\n#     print(\"Starting fine-tuning...\")\n#     trainer.train()\n    \n#     # Save the fine-tuned model\n#     trainer.save_model()\n#     tokenizer.save_pretrained(output_dir)\n    \n#     return trainer\n\n# def evaluate_model(model, tokenizer, test_dataset, device):\n#     \"\"\"Comprehensive evaluation of the fine-tuned model\"\"\"\n#     print(\"Evaluating fine-tuned model...\")\n    \n#     # Create test dataloader\n#     test_dataloader = DataLoader(\n#         test_dataset, \n#         batch_size=8, \n#         shuffle=False,\n#         collate_fn=lambda x: {\n#             'input_ids': torch.nn.utils.rnn.pad_sequence(\n#                 [item['input_ids'] for item in x], \n#                 batch_first=True, \n#                 padding_value=tokenizer.pad_token_id\n#             ),\n#             'attention_mask': torch.nn.utils.rnn.pad_sequence(\n#                 [item['attention_mask'] for item in x], \n#                 batch_first=True, \n#                 padding_value=0\n#             )\n#         }\n#     )\n    \n#     # Initialize evaluator\n#     evaluator = NWPEvaluator(model, tokenizer, device)\n    \n#     # Calculate perplexity\n#     perplexity = evaluator.calculate_perplexity(test_dataloader)\n#     print(f\"Perplexity: {perplexity:.2f}\")\n    \n#     # Calculate top-k accuracy\n#     accuracies = evaluator.calculate_top_k_accuracy(test_dataloader, k_values=[1, 5, 10])\n#     print(\"Top-k Accuracies:\")\n#     for k, acc in accuracies.items():\n#         print(f\"  Top-{k}: {acc:.4f}\")\n    \n#     return perplexity, accuracies, evaluator\n\n# def inspect_training_examples(dataset, tokenizer, num_examples=5, max_display_length=100):\n#     \"\"\"\n#     Inspect and visualize training examples to understand the data format\n    \n#     Args:\n#         dataset: WikiTextDataset instance\n#         tokenizer: GPT2Tokenizer instance\n#         num_examples: Number of examples to display\n#         max_display_length: Maximum length of text to display\n#     \"\"\"\n#     print(\"=\" * 80)\n#     print(\"TRAINING EXAMPLES INSPECTION\")\n#     print(\"=\" * 80)\n    \n#     print(f\"Dataset Info:\")\n#     print(f\"  Total examples: {len(dataset)}\")\n#     print(f\"  Max sequence length: {dataset.max_length}\")\n#     print(f\"  Tokenizer vocab size: {len(tokenizer)}\")\n#     print(f\"  Tokenizer pad token: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})\")\n#     print(f\"  Tokenizer eos token: '{tokenizer.eos_token}' (ID: {tokenizer.eos_token_id})\")\n#     print()\n    \n#     for i in range(min(num_examples, len(dataset))):\n#         print(f\"EXAMPLE {i+1}:\")\n#         print(\"-\" * 60)\n        \n#         # Get the raw example\n#         example = dataset[i]\n#         input_ids = example['input_ids']\n#         attention_mask = example['attention_mask']\n        \n#         # Basic statistics\n#         print(f\"Sequence length: {len(input_ids)}\")\n#         print(f\"Attention mask sum: {attention_mask.sum().item()}\")\n#         print()\n        \n#         # Convert tokens back to text\n#         full_text = tokenizer.decode(input_ids, skip_special_tokens=False)\n#         clean_text = tokenizer.decode(input_ids, skip_special_tokens=True)\n        \n#         # Display text (truncated if too long)\n#         if len(full_text) > max_display_length:\n#             display_text = full_text[:max_display_length] + \"...\"\n#             clean_display = clean_text[:max_display_length] + \"...\"\n#         else:\n#             display_text = full_text\n#             clean_display = clean_text\n        \n#         print(f\"Full text (with special tokens):\")\n#         print(f\"  '{display_text}'\")\n#         print()\n#         print(f\"Clean text (without special tokens):\")\n#         print(f\"  '{clean_display}'\")\n#         print()\n        \n#         # Show first 20 tokens in detail\n#         print(\"First 20 tokens breakdown:\")\n#         print(\"  Token ID | Token Text | Attention\")\n#         print(\"  ---------|------------|----------\")\n        \n#         for j in range(min(20, len(input_ids))):\n#             token_id = input_ids[j].item()\n#             token_text = tokenizer.decode([token_id])\n#             attention = attention_mask[j].item()\n            \n#             # Handle special characters for display\n#             if token_text == tokenizer.eos_token:\n#                 token_display = \"<EOS>\"\n#             elif token_text == tokenizer.pad_token:\n#                 token_display = \"<PAD>\"\n#             elif token_text.strip() == \"\":\n#                 token_display = \"<SPACE>\"\n#             else:\n#                 token_display = repr(token_text)\n            \n#             print(f\"  {token_id:8d} | {token_display:10s} | {attention:9d}\")\n        \n#         if len(input_ids) > 20:\n#             print(f\"  ... ({len(input_ids) - 20} more tokens)\")\n        \n#         print()\n#         print(\"=\" * 60)\n#         print()\n\n# def show_next_word_prediction_examples(dataset, tokenizer, num_examples=3):\n#     \"\"\"\n#     Show how the training examples work for next word prediction\n    \n#     Args:\n#         dataset: WikiTextDataset instance\n#         tokenizer: GPT2Tokenizer instance\n#         num_examples: Number of examples to show\n#     \"\"\"\n#     print(\"=\" * 80)\n#     print(\"NEXT WORD PREDICTION TRAINING FORMAT\")\n#     print(\"=\" * 80)\n    \n#     print(\"This shows how each sequence is used for next word prediction training.\")\n#     print(\"For each position, the model sees all previous tokens and predicts the next one.\")\n#     print()\n    \n#     for i in range(min(num_examples, len(dataset))):\n#         print(f\"EXAMPLE {i+1}:\")\n#         print(\"-\" * 60)\n        \n#         example = dataset[i]\n#         input_ids = example['input_ids']\n        \n#         # Show first 10 positions as input -> target pairs\n#         print(\"Input Context -> Target Token\")\n#         print(\"-\" * 40)\n        \n#         for pos in range(1, min(11, len(input_ids))):\n#             # Context is everything before current position\n#             context_ids = input_ids[:pos]\n#             target_id = input_ids[pos]\n            \n#             # Convert to text\n#             context_text = tokenizer.decode(context_ids, skip_special_tokens=True)\n#             target_text = tokenizer.decode([target_id], skip_special_tokens=True)\n            \n#             # Truncate context if too long\n#             if len(context_text) > 50:\n#                 context_display = \"...\" + context_text[-47:]\n#             else:\n#                 context_display = context_text\n            \n#             print(f\"'{context_display}' -> '{target_text}'\")\n        \n#         if len(input_ids) > 11:\n#             print(f\"... ({len(input_ids) - 11} more prediction pairs)\")\n        \n#         print()\n#         print(\"=\" * 60)\n#         print()\n\n# def analyze_dataset_statistics(dataset, tokenizer):\n#     \"\"\"\n#     Analyze statistical properties of the dataset\n    \n#     Args:\n#         dataset: WikiTextDataset instance\n#         tokenizer: GPT2Tokenizer instance\n#     \"\"\"\n#     print(\"=\" * 80)\n#     print(\"DATASET STATISTICAL ANALYSIS\")\n#     print(\"=\" * 80)\n    \n#     sequence_lengths = []\n#     token_counts = {}\n    \n#     print(\"Analyzing dataset... (this may take a moment)\")\n    \n#     for i in tqdm(range(len(dataset)), desc=\"Processing examples\"):\n#         example = dataset[i]\n#         input_ids = example['input_ids']\n        \n#         # Collect sequence length\n#         sequence_lengths.append(len(input_ids))\n        \n#         # Count token frequencies (sample first 1000 examples to save time)\n#         if i < 1000:\n#             for token_id in input_ids:\n#                 token_id = token_id.item()\n#                 token_counts[token_id] = token_counts.get(token_id, 0) + 1\n    \n#     # Sequence length statistics\n#     sequence_lengths = np.array(sequence_lengths)\n#     print(f\"Sequence Length Statistics:\")\n#     print(f\"  Mean: {sequence_lengths.mean():.2f}\")\n#     print(f\"  Median: {np.median(sequence_lengths):.2f}\")\n#     print(f\"  Min: {sequence_lengths.min()}\")\n#     print(f\"  Max: {sequence_lengths.max()}\")\n#     print(f\"  Std: {sequence_lengths.std():.2f}\")\n#     print()\n    \n#     # Token frequency analysis (top 20 most common tokens)\n#     if token_counts:\n#         print(\"Top 20 Most Common Tokens (from first 1000 examples):\")\n#         print(\"  Rank | Token ID | Count | Token Text\")\n#         print(\"  -----|----------|-------|----------\")\n        \n#         sorted_tokens = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n#         for rank, (token_id, count) in enumerate(sorted_tokens[:20], 1):\n#             token_text = tokenizer.decode([token_id])\n            \n#             # Handle special characters\n#             if token_text == tokenizer.eos_token:\n#                 token_display = \"<EOS>\"\n#             elif token_text == tokenizer.pad_token:\n#                 token_display = \"<PAD>\"\n#             elif token_text.strip() == \"\":\n#                 token_display = \"<SPACE>\"\n#             else:\n#                 token_display = repr(token_text)\n            \n#             print(f\"  {rank:4d} | {token_id:8d} | {count:5d} | {token_display}\")\n        \n#         print(f\"\\nTotal unique tokens in sample: {len(token_counts)}\")\n    \n#     print()\n#     print(\"=\" * 80)\n\n# def demonstrate_predictions(evaluator, tokenizer):\n#     \"\"\"Demonstrate model predictions on sample texts\"\"\"\n#     print(\"\\nSample Predictions:\")\n#     print(\"=\" * 50)\n    \n#     sample_texts = [\n#         \"Albert Einstein was a\",\n#         \"The capital of France is\",\n#         \"Machine learning is the\",\n#         \"In the year 2024, artificial intelligence\",\n#         \"The quick brown fox\"\n#     ]\n    \n#     for text in sample_texts:\n#         predictions = evaluator.sample_predictions(text, num_predictions=3)\n#         print(f\"Input: '{text}'\")\n#         print(f\"Predictions: {predictions}\")\n#         print(\"-\" * 30)\n\n# def main():\n#     # Set device\n#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#     print(f\"Using device: {device}\")\n    \n#     # Load data\n#     train_texts, valid_texts, test_texts = load_and_prepare_data()\n    \n#     # Setup model and tokenizer\n#     model, tokenizer = setup_model_and_tokenizer()\n#     model.to(device)\n    \n#     # Create datasets\n#     print(\"Creating datasets...\")\n#     train_dataset = WikiTextDataset(train_texts[:1000], tokenizer)  # Subset for demo\n#     eval_dataset = WikiTextDataset(valid_texts[:200], tokenizer)\n#     test_dataset = WikiTextDataset(test_texts[:200], tokenizer)\n    \n#     # Fine-tune model\n#     trainer = fine_tune_model(model, tokenizer, train_dataset, eval_dataset)\n    \n#     # Evaluate model\n#     perplexity, accuracies, evaluator = evaluate_model(model, tokenizer, test_dataset, device)\n    \n#     # Demonstrate predictions\n#     demonstrate_predictions(evaluator, tokenizer)\n    \n#     # Create results summary\n#     results = {\n#         'Metric': ['Perplexity', 'Top-1 Accuracy', 'Top-5 Accuracy', 'Top-10 Accuracy'],\n#         'Value': [f\"{perplexity:.2f}\", f\"{accuracies[1]:.4f}\", f\"{accuracies[5]:.4f}\", f\"{accuracies[10]:.4f}\"]\n#     }\n    \n#     results_df = pd.DataFrame(results)\n#     print(\"\\nFinal Results Summary:\")\n#     print(results_df.to_string(index=False))\n    \n#     return model, tokenizer, results_df\n\n# def explore_training_data():\n#     \"\"\"\n#     Comprehensive exploration of training data\n#     Run this before training to understand your dataset\n#     \"\"\"\n#     print(\"🔍 EXPLORING TRAINING DATA\")\n#     print(\"=\" * 80)\n    \n#     # Load a small sample of data for exploration\n#     print(\"Loading sample data...\")\n#     train_texts, valid_texts, test_texts = load_and_prepare_data()\n    \n#     # Setup tokenizer\n#     model, tokenizer = setup_model_and_tokenizer()\n    \n#     # Create small datasets for exploration\n#     sample_train = WikiTextDataset(train_texts[:50], tokenizer)  # Small sample\n#     sample_valid = WikiTextDataset(valid_texts[:20], tokenizer)\n    \n#     print(f\"Created sample datasets:\")\n#     print(f\"  Sample train: {len(sample_train)} examples\")\n#     print(f\"  Sample valid: {len(sample_valid)} examples\")\n#     print()\n    \n#     # 1. Inspect raw training examples\n#     print(\"1️⃣ INSPECTING RAW TRAINING EXAMPLES\")\n#     inspect_training_examples(sample_train, tokenizer, num_examples=3)\n    \n#     # 2. Show next word prediction format\n#     print(\"2️⃣ NEXT WORD PREDICTION FORMAT\")\n#     show_next_word_prediction_examples(sample_train, tokenizer, num_examples=2)\n    \n#     # 3. Dataset statistics\n#     print(\"3️⃣ DATASET STATISTICS\")\n#     analyze_dataset_statistics(sample_train, tokenizer)\n    \n#     # 4. Show some original WikiText content\n#     print(\"4️⃣ SAMPLE WIKITEXT CONTENT\")\n#     print(\"=\" * 40)\n#     print(\"Here are some original texts from WikiText-2:\")\n#     print()\n    \n#     for i, text in enumerate(train_texts[:3]):\n#         if len(text.strip()) > 0:\n#             print(f\"Text {i+1}:\")\n#             print(f\"  Length: {len(text)} characters\")\n#             print(f\"  Preview: {text[:200]}...\")\n#             print()\n    \n#     return sample_train, sample_valid, tokenizer\n\n# if __name__ == \"__main__\":\n#     # Note: This is a comprehensive implementation\n#     # For actual execution, you may want to run sections separately\n#     # due to computational requirements\n    \n#     print(\"GPT-2 Fine-tuning Pipeline for Next Word Prediction\")\n#     print(\"=\" * 60)\n    \n#     # Uncomment to explore training data first\n#     # sample_train, sample_valid, tokenizer = explore_training_data()\n    \n#     # Uncomment the following line to run the full pipeline\n#     model, tokenizer, results = main()\n    \n#     print(\"Pipeline setup complete!\")\n#     print(\"To explore data: uncomment explore_training_data() call\")\n#     print(\"To run training: uncomment the main() call and execute with sufficient GPU memory\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T22:07:33.071353Z","iopub.execute_input":"2025-07-08T22:07:33.071674Z","iopub.status.idle":"2025-07-08T22:48:05.386918Z","shell.execute_reply.started":"2025-07-08T22:07:33.071647Z","shell.execute_reply":"2025-07-08T22:48:05.386120Z"}},"outputs":[{"name":"stdout","text":"GPT-2 Fine-tuning Pipeline for Next Word Prediction\n============================================================\nUsing device: cuda\nLoading WikiText-2 dataset...\nTrain texts: 23767\nValidation texts: 2461\nTest texts: 2891\nSetting up GPT-2 model and tokenizer...\nCreating datasets...\nProcessing texts...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1000/1000 [00:00<00:00, 1044.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processing texts...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 200/200 [00:00<00:00, 1230.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processing texts...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 200/200 [00:00<00:00, 1128.40it/s]\n/tmp/ipykernel_36/3394084516.py:217: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"name":"stdout","text":"Starting fine-tuning...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3400' max='3400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3400/3400 35:15, Epoch 50/50]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.512800</td>\n      <td>3.413807</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.211400</td>\n      <td>3.653531</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.842600</td>\n      <td>4.139855</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.599100</td>\n      <td>4.474370</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.513800</td>\n      <td>4.677817</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.434100</td>\n      <td>4.832667</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Evaluating fine-tuned model...\n","output_type":"stream"},{"name":"stderr","text":"Calculating perplexity: 100%|██████████| 27/27 [00:04<00:00,  6.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity: 262906461.75\n","output_type":"stream"},{"name":"stderr","text":"Calculating top-k accuracy: 100%|██████████| 27/27 [05:05<00:00, 11.30s/it]","output_type":"stream"},{"name":"stdout","text":"Top-k Accuracies:\n  Top-1: 0.3412\n  Top-5: 0.5427\n  Top-10: 0.6134\n\nSample Predictions:\n==================================================\nInput: 'Albert Einstein was a'\nPredictions: [' devout', ' Catholic', ' and']\n------------------------------\nInput: 'The capital of France is'\nPredictions: [' Br', 'ind', 'isi']\n------------------------------\nInput: 'Machine learning is the'\nPredictions: [' art', ' of', ' training']\n------------------------------\nInput: 'In the year 2024, artificial intelligence'\nPredictions: [' (', ' AI', ' )']\n------------------------------\nInput: 'The quick brown fox'\nPredictions: [' was', ' once', ' believed']\n------------------------------\n\nFinal Results Summary:\n         Metric        Value\n     Perplexity 262906461.75\n Top-1 Accuracy       0.3412\n Top-5 Accuracy       0.5427\nTop-10 Accuracy       0.6134\nPipeline setup complete!\nTo explore data: uncomment explore_training_data() call\nTo run training: uncomment the main() call and execute with sufficient GPU memory\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# from torch.utils.data import Dataset, DataLoader\n# from transformers import (\n#     GPT2LMHeadModel, \n#     GPT2Tokenizer, \n#     TrainingArguments, \n#     Trainer,\n#     DataCollatorForLanguageModeling\n# )\n# from datasets import load_dataset\n# import numpy as np\n# from tqdm import tqdm\n# import matplotlib.pyplot as plt\n# import pandas as pd\n# from typing import Dict, List, Tuple\n# import math\n\n# class WikiTextDataset(Dataset):\n#     \"\"\"Custom dataset for WikiText-2 with proper tokenization\"\"\"\n    \n#     def __init__(self, texts: List[str], tokenizer, max_length: int = 512):\n#         self.tokenizer = tokenizer\n#         self.max_length = max_length\n#         self.examples = []\n        \n#         print(\"Processing texts...\")\n#         for text in tqdm(texts):\n#             if len(text.strip()) > 0:  # Skip empty texts\n#                 # Tokenize and create sliding windows\n#                 tokens = tokenizer.encode(text, add_special_tokens=True)\n                \n#                 # Create overlapping sequences\n#                 for i in range(0, len(tokens) - 1, max_length // 2):\n#                     chunk = tokens[i:i + max_length]\n#                     if len(chunk) > 1:  # Need at least 2 tokens (input + target)\n#                         self.examples.append(chunk)\n    \n#     def __len__(self):\n#         return len(self.examples)\n    \n#     def __getitem__(self, idx):\n#         return {\n#             'input_ids': torch.tensor(self.examples[idx], dtype=torch.long),\n#             'attention_mask': torch.ones(len(self.examples[idx]), dtype=torch.long)\n#         }\n\n# class NWPEvaluator:\n#     \"\"\"Evaluator for Next Word Prediction metrics\"\"\"\n    \n#     def __init__(self, model, tokenizer, device):\n#         self.model = model\n#         self.tokenizer = tokenizer\n#         self.device = device\n#         self.model.eval()\n    \n#     def calculate_perplexity(self, dataloader) -> float:\n#         \"\"\"Calculate perplexity on evaluation dataset\"\"\"\n#         total_loss = 0\n#         total_tokens = 0\n        \n#         with torch.no_grad():\n#             for batch in tqdm(dataloader, desc=\"Calculating perplexity\"):\n#                 input_ids = batch['input_ids'].to(self.device)\n#                 attention_mask = batch['attention_mask'].to(self.device)\n                \n#                 # Shift labels for next token prediction\n#                 labels = input_ids.clone()\n#                 labels[:, :-1] = input_ids[:, 1:]\n#                 labels[:, -1] = -100  # Ignore last token\n                \n#                 outputs = self.model(\n#                     input_ids=input_ids,\n#                     attention_mask=attention_mask,\n#                     labels=labels\n#                 )\n                \n#                 loss = outputs.loss\n#                 # Count valid tokens (not -100)\n#                 valid_tokens = (labels != -100).sum().item()\n                \n#                 total_loss += loss.item() * valid_tokens\n#                 total_tokens += valid_tokens\n        \n#         avg_loss = total_loss / total_tokens\n#         perplexity = math.exp(avg_loss)\n#         return perplexity\n    \n#     def calculate_top_k_accuracy(self, dataloader, k_values: List[int] = [1, 5, 10]) -> Dict[int, float]:\n#         \"\"\"Calculate top-k accuracy for next word prediction\"\"\"\n#         correct_predictions = {k: 0 for k in k_values}\n#         total_predictions = 0\n        \n#         with torch.no_grad():\n#             for batch in tqdm(dataloader, desc=\"Calculating top-k accuracy\"):\n#                 input_ids = batch['input_ids'].to(self.device)\n#                 attention_mask = batch['attention_mask'].to(self.device)\n                \n#                 for seq_idx in range(input_ids.size(0)):\n#                     sequence = input_ids[seq_idx]\n#                     mask = attention_mask[seq_idx]\n                    \n#                     # Get valid length\n#                     valid_length = mask.sum().item()\n                    \n#                     # Predict each position (except last)\n#                     for pos in range(1, valid_length):\n#                         context = sequence[:pos].unsqueeze(0)\n#                         target = sequence[pos].item()\n                        \n#                         outputs = self.model(context)\n#                         logits = outputs.logits[0, -1, :]  # Last position logits\n                        \n#                         # Get top-k predictions\n#                         top_k_max = max(k_values)\n#                         top_k_tokens = torch.topk(logits, top_k_max).indices\n                        \n#                         # Check accuracy for different k values\n#                         for k in k_values:\n#                             if target in top_k_tokens[:k]:\n#                                 correct_predictions[k] += 1\n                        \n#                         total_predictions += 1\n        \n#         # Calculate accuracies\n#         accuracies = {k: correct_predictions[k] / total_predictions for k in k_values}\n#         return accuracies\n    \n#     def sample_predictions(self, text: str, num_predictions: int = 5) -> List[str]:\n#         \"\"\"Generate sample predictions for demonstration\"\"\"\n#         tokens = self.tokenizer.encode(text, return_tensors='pt').to(self.device)\n        \n#         predictions = []\n#         with torch.no_grad():\n#             for _ in range(num_predictions):\n#                 outputs = self.model(tokens)\n#                 logits = outputs.logits[0, -1, :]\n                \n#                 # Sample from distribution\n#                 probs = torch.softmax(logits, dim=-1)\n#                 next_token = torch.multinomial(probs, 1)\n                \n#                 predicted_word = self.tokenizer.decode(next_token.item())\n#                 predictions.append(predicted_word)\n                \n#                 # Add to context for next prediction\n#                 tokens = torch.cat([tokens, next_token.unsqueeze(0)], dim=1)\n        \n#         return predictions\n\n# def load_and_prepare_data():\n#     \"\"\"Load WikiText-2 dataset and prepare for training\"\"\"\n#     print(\"Loading WikiText-2 dataset...\")\n#     dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n    \n#     # Filter out empty texts\n#     train_texts = [text for text in dataset['train']['text'] if len(text.strip()) > 0]\n#     valid_texts = [text for text in dataset['validation']['text'] if len(text.strip()) > 0]\n#     test_texts = [text for text in dataset['test']['text'] if len(text.strip()) > 0]\n    \n#     print(f\"Train texts: {len(train_texts)}\")\n#     print(f\"Validation texts: {len(valid_texts)}\")\n#     print(f\"Test texts: {len(test_texts)}\")\n    \n#     return train_texts, valid_texts, test_texts\n\n# def setup_model_and_tokenizer():\n#     \"\"\"Initialize GPT-2 model and tokenizer\"\"\"\n#     print(\"Setting up GPT-2 model and tokenizer...\")\n#     model_name = \"gpt2\"\n    \n#     tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n#     model = GPT2LMHeadModel.from_pretrained(model_name)\n    \n#     # Add padding token\n#     tokenizer.pad_token = tokenizer.eos_token\n    \n#     return model, tokenizer\n\n# def fine_tune_model(model, tokenizer, train_dataset, eval_dataset, output_dir=\"./gpt2-wikitext2\"):\n#     \"\"\"Fine-tune GPT-2 on WikiText-2\"\"\"\n    \n#     # Data collator for language modeling\n#     data_collator = DataCollatorForLanguageModeling(\n#         tokenizer=tokenizer,\n#         mlm=False,  # GPT-2 is autoregressive, not masked LM\n#         pad_to_multiple_of=8,\n#         return_tensors=\"pt\"\n#     )\n    \n#     # Training arguments\n#     training_args = TrainingArguments(\n#         output_dir=output_dir,\n#         overwrite_output_dir=True,\n#         num_train_epochs=3,\n#         per_device_train_batch_size=4,\n#         per_device_eval_batch_size=4,\n#         warmup_steps=500,\n#         logging_steps=100,\n#         save_steps=1000,\n#         eval_steps=500,\n#         eval_strategy=\"steps\",  # Changed from evaluation_strategy\n#         load_best_model_at_end=True,\n#         save_total_limit=2,\n#         gradient_accumulation_steps=2,\n#         fp16=True,  # Mixed precision training\n#         dataloader_pin_memory=True,\n#         learning_rate=5e-5,\n#         weight_decay=0.01,\n#         adam_epsilon=1e-8,\n#         max_grad_norm=1.0,\n#         lr_scheduler_type=\"linear\",\n#         report_to=[],  # Changed from None to empty list\n#     )\n    \n#     # Initialize trainer\n#     trainer = Trainer(\n#         model=model,\n#         args=training_args,\n#         data_collator=data_collator,\n#         train_dataset=train_dataset,\n#         eval_dataset=eval_dataset,\n#         tokenizer=tokenizer,\n#     )\n    \n#     # Train the model\n#     print(\"Starting fine-tuning...\")\n#     trainer.train()\n    \n#     # Save the fine-tuned model\n#     trainer.save_model()\n#     tokenizer.save_pretrained(output_dir)\n    \n#     return trainer\n\n# def evaluate_model(model, tokenizer, test_dataset, device):\n#     \"\"\"Comprehensive evaluation of the fine-tuned model\"\"\"\n#     print(\"Evaluating fine-tuned model...\")\n    \n#     # Create test dataloader\n#     test_dataloader = DataLoader(\n#         test_dataset, \n#         batch_size=8, \n#         shuffle=False,\n#         collate_fn=lambda x: {\n#             'input_ids': torch.nn.utils.rnn.pad_sequence(\n#                 [item['input_ids'] for item in x], \n#                 batch_first=True, \n#                 padding_value=tokenizer.pad_token_id\n#             ),\n#             'attention_mask': torch.nn.utils.rnn.pad_sequence(\n#                 [item['attention_mask'] for item in x], \n#                 batch_first=True, \n#                 padding_value=0\n#             )\n#         }\n#     )\n    \n#     # Initialize evaluator\n#     evaluator = NWPEvaluator(model, tokenizer, device)\n    \n#     # Calculate perplexity\n#     perplexity = evaluator.calculate_perplexity(test_dataloader)\n#     print(f\"Perplexity: {perplexity:.2f}\")\n    \n#     # Calculate top-k accuracy\n#     accuracies = evaluator.calculate_top_k_accuracy(test_dataloader, k_values=[1, 5, 10])\n#     print(\"Top-k Accuracies:\")\n#     for k, acc in accuracies.items():\n#         print(f\"  Top-{k}: {acc:.4f}\")\n    \n#     return perplexity, accuracies, evaluator\n\n# def inspect_training_examples(dataset, tokenizer, num_examples=5, max_display_length=100):\n#     \"\"\"\n#     Inspect and visualize training examples to understand the data format\n    \n#     Args:\n#         dataset: WikiTextDataset instance\n#         tokenizer: GPT2Tokenizer instance\n#         num_examples: Number of examples to display\n#         max_display_length: Maximum length of text to display\n#     \"\"\"\n#     print(\"=\" * 80)\n#     print(\"TRAINING EXAMPLES INSPECTION\")\n#     print(\"=\" * 80)\n    \n#     print(f\"Dataset Info:\")\n#     print(f\"  Total examples: {len(dataset)}\")\n#     print(f\"  Max sequence length: {dataset.max_length}\")\n#     print(f\"  Tokenizer vocab size: {len(tokenizer)}\")\n#     print(f\"  Tokenizer pad token: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})\")\n#     print(f\"  Tokenizer eos token: '{tokenizer.eos_token}' (ID: {tokenizer.eos_token_id})\")\n#     print()\n    \n#     for i in range(min(num_examples, len(dataset))):\n#         print(f\"EXAMPLE {i+1}:\")\n#         print(\"-\" * 60)\n        \n#         # Get the raw example\n#         example = dataset[i]\n#         input_ids = example['input_ids']\n#         attention_mask = example['attention_mask']\n        \n#         # Basic statistics\n#         print(f\"Sequence length: {len(input_ids)}\")\n#         print(f\"Attention mask sum: {attention_mask.sum().item()}\")\n#         print()\n        \n#         # Convert tokens back to text\n#         full_text = tokenizer.decode(input_ids, skip_special_tokens=False)\n#         clean_text = tokenizer.decode(input_ids, skip_special_tokens=True)\n        \n#         # Display text (truncated if too long)\n#         if len(full_text) > max_display_length:\n#             display_text = full_text[:max_display_length] + \"...\"\n#             clean_display = clean_text[:max_display_length] + \"...\"\n#         else:\n#             display_text = full_text\n#             clean_display = clean_text\n        \n#         print(f\"Full text (with special tokens):\")\n#         print(f\"  '{display_text}'\")\n#         print()\n#         print(f\"Clean text (without special tokens):\")\n#         print(f\"  '{clean_display}'\")\n#         print()\n        \n#         # Show first 20 tokens in detail\n#         print(\"First 20 tokens breakdown:\")\n#         print(\"  Token ID | Token Text | Attention\")\n#         print(\"  ---------|------------|----------\")\n        \n#         for j in range(min(20, len(input_ids))):\n#             token_id = input_ids[j].item()\n#             token_text = tokenizer.decode([token_id])\n#             attention = attention_mask[j].item()\n            \n#             # Handle special characters for display\n#             if token_text == tokenizer.eos_token:\n#                 token_display = \"<EOS>\"\n#             elif token_text == tokenizer.pad_token:\n#                 token_display = \"<PAD>\"\n#             elif token_text.strip() == \"\":\n#                 token_display = \"<SPACE>\"\n#             else:\n#                 token_display = repr(token_text)\n            \n#             print(f\"  {token_id:8d} | {token_display:10s} | {attention:9d}\")\n        \n#         if len(input_ids) > 20:\n#             print(f\"  ... ({len(input_ids) - 20} more tokens)\")\n        \n#         print()\n#         print(\"=\" * 60)\n#         print()\n\n# def show_next_word_prediction_examples(dataset, tokenizer, num_examples=3):\n#     \"\"\"\n#     Show how the training examples work for next word prediction\n    \n#     Args:\n#         dataset: WikiTextDataset instance\n#         tokenizer: GPT2Tokenizer instance\n#         num_examples: Number of examples to show\n#     \"\"\"\n#     print(\"=\" * 80)\n#     print(\"NEXT WORD PREDICTION TRAINING FORMAT\")\n#     print(\"=\" * 80)\n    \n#     print(\"This shows how each sequence is used for next word prediction training.\")\n#     print(\"For each position, the model sees all previous tokens and predicts the next one.\")\n#     print()\n    \n#     for i in range(min(num_examples, len(dataset))):\n#         print(f\"EXAMPLE {i+1}:\")\n#         print(\"-\" * 60)\n        \n#         example = dataset[i]\n#         input_ids = example['input_ids']\n        \n#         # Show first 10 positions as input -> target pairs\n#         print(\"Input Context -> Target Token\")\n#         print(\"-\" * 40)\n        \n#         for pos in range(1, min(11, len(input_ids))):\n#             # Context is everything before current position\n#             context_ids = input_ids[:pos]\n#             target_id = input_ids[pos]\n            \n#             # Convert to text\n#             context_text = tokenizer.decode(context_ids, skip_special_tokens=True)\n#             target_text = tokenizer.decode([target_id], skip_special_tokens=True)\n            \n#             # Truncate context if too long\n#             if len(context_text) > 50:\n#                 context_display = \"...\" + context_text[-47:]\n#             else:\n#                 context_display = context_text\n            \n#             print(f\"'{context_display}' -> '{target_text}'\")\n        \n#         if len(input_ids) > 11:\n#             print(f\"... ({len(input_ids) - 11} more prediction pairs)\")\n        \n#         print()\n#         print(\"=\" * 60)\n#         print()\n\n# def analyze_dataset_statistics(dataset, tokenizer):\n#     \"\"\"\n#     Analyze statistical properties of the dataset\n    \n#     Args:\n#         dataset: WikiTextDataset instance\n#         tokenizer: GPT2Tokenizer instance\n#     \"\"\"\n#     print(\"=\" * 80)\n#     print(\"DATASET STATISTICAL ANALYSIS\")\n#     print(\"=\" * 80)\n    \n#     sequence_lengths = []\n#     token_counts = {}\n    \n#     print(\"Analyzing dataset... (this may take a moment)\")\n    \n#     for i in tqdm(range(len(dataset)), desc=\"Processing examples\"):\n#         example = dataset[i]\n#         input_ids = example['input_ids']\n        \n#         # Collect sequence length\n#         sequence_lengths.append(len(input_ids))\n        \n#         # Count token frequencies (sample first 1000 examples to save time)\n#         if i < 1000:\n#             for token_id in input_ids:\n#                 token_id = token_id.item()\n#                 token_counts[token_id] = token_counts.get(token_id, 0) + 1\n    \n#     # Sequence length statistics\n#     sequence_lengths = np.array(sequence_lengths)\n#     print(f\"Sequence Length Statistics:\")\n#     print(f\"  Mean: {sequence_lengths.mean():.2f}\")\n#     print(f\"  Median: {np.median(sequence_lengths):.2f}\")\n#     print(f\"  Min: {sequence_lengths.min()}\")\n#     print(f\"  Max: {sequence_lengths.max()}\")\n#     print(f\"  Std: {sequence_lengths.std():.2f}\")\n#     print()\n    \n#     # Token frequency analysis (top 20 most common tokens)\n#     if token_counts:\n#         print(\"Top 20 Most Common Tokens (from first 1000 examples):\")\n#         print(\"  Rank | Token ID | Count | Token Text\")\n#         print(\"  -----|----------|-------|----------\")\n        \n#         sorted_tokens = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n#         for rank, (token_id, count) in enumerate(sorted_tokens[:20], 1):\n#             token_text = tokenizer.decode([token_id])\n            \n#             # Handle special characters\n#             if token_text == tokenizer.eos_token:\n#                 token_display = \"<EOS>\"\n#             elif token_text == tokenizer.pad_token:\n#                 token_display = \"<PAD>\"\n#             elif token_text.strip() == \"\":\n#                 token_display = \"<SPACE>\"\n#             else:\n#                 token_display = repr(token_text)\n            \n#             print(f\"  {rank:4d} | {token_id:8d} | {count:5d} | {token_display}\")\n        \n#         print(f\"\\nTotal unique tokens in sample: {len(token_counts)}\")\n    \n#     print()\n#     print(\"=\" * 80)\n\n# def demonstrate_predictions(evaluator, tokenizer):\n#     \"\"\"Demonstrate model predictions on sample texts\"\"\"\n#     print(\"\\nSample Predictions:\")\n#     print(\"=\" * 50)\n    \n#     sample_texts = [\n#         \"Albert Einstein was a\",\n#         \"The capital of France is\",\n#         \"Machine learning is the\",\n#         \"In the year 2024, artificial intelligence\",\n#         \"The quick brown fox\"\n#     ]\n    \n#     for text in sample_texts:\n#         predictions = evaluator.sample_predictions(text, num_predictions=3)\n#         print(f\"Input: '{text}'\")\n#         print(f\"Predictions: {predictions}\")\n#         print(\"-\" * 30)\n\n# def main():\n#     # Set device\n#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#     print(f\"Using device: {device}\")\n    \n#     # Load data\n#     train_texts, valid_texts, test_texts = load_and_prepare_data()\n    \n#     # Setup model and tokenizer\n#     model, tokenizer = setup_model_and_tokenizer()\n#     model.to(device)\n    \n#     # Create datasets\n#     print(\"Creating datasets...\")\n#     train_dataset = WikiTextDataset(train_texts[:1000], tokenizer)  # Subset for demo\n#     eval_dataset = WikiTextDataset(valid_texts[:200], tokenizer)\n#     test_dataset = WikiTextDataset(test_texts[:200], tokenizer)\n    \n#     # Fine-tune model\n#     trainer = fine_tune_model(model, tokenizer, train_dataset, eval_dataset)\n    \n#     # Evaluate model\n#     perplexity, accuracies, evaluator = evaluate_model(model, tokenizer, test_dataset, device)\n    \n#     # Demonstrate predictions\n#     demonstrate_predictions(evaluator, tokenizer)\n    \n#     # Create results summary\n#     results = {\n#         'Metric': ['Perplexity', 'Top-1 Accuracy', 'Top-5 Accuracy', 'Top-10 Accuracy'],\n#         'Value': [f\"{perplexity:.2f}\", f\"{accuracies[1]:.4f}\", f\"{accuracies[5]:.4f}\", f\"{accuracies[10]:.4f}\"]\n#     }\n    \n#     results_df = pd.DataFrame(results)\n#     print(\"\\nFinal Results Summary:\")\n#     print(results_df.to_string(index=False))\n    \n#     return model, tokenizer, results_df\n\n# def explore_training_data():\n#     \"\"\"\n#     Comprehensive exploration of training data\n#     Run this before training to understand your dataset\n#     \"\"\"\n#     print(\"🔍 EXPLORING TRAINING DATA\")\n#     print(\"=\" * 80)\n    \n#     # Load a small sample of data for exploration\n#     print(\"Loading sample data...\")\n#     train_texts, valid_texts, test_texts = load_and_prepare_data()\n    \n#     # Setup tokenizer\n#     model, tokenizer = setup_model_and_tokenizer()\n    \n#     # Create small datasets for exploration\n#     sample_train = WikiTextDataset(train_texts[:50], tokenizer)  # Small sample\n#     sample_valid = WikiTextDataset(valid_texts[:20], tokenizer)\n    \n#     print(f\"Created sample datasets:\")\n#     print(f\"  Sample train: {len(sample_train)} examples\")\n#     print(f\"  Sample valid: {len(sample_valid)} examples\")\n#     print()\n    \n#     # 1. Inspect raw training examples\n#     print(\"1️⃣ INSPECTING RAW TRAINING EXAMPLES\")\n#     inspect_training_examples(sample_train, tokenizer, num_examples=3)\n    \n#     # 2. Show next word prediction format\n#     print(\"2️⃣ NEXT WORD PREDICTION FORMAT\")\n#     show_next_word_prediction_examples(sample_train, tokenizer, num_examples=2)\n    \n#     # 3. Dataset statistics\n#     print(\"3️⃣ DATASET STATISTICS\")\n#     analyze_dataset_statistics(sample_train, tokenizer)\n    \n#     # 4. Show some original WikiText content\n#     print(\"4️⃣ SAMPLE WIKITEXT CONTENT\")\n#     print(\"=\" * 40)\n#     print(\"Here are some original texts from WikiText-2:\")\n#     print()\n    \n#     for i, text in enumerate(train_texts[:3]):\n#         if len(text.strip()) > 0:\n#             print(f\"Text {i+1}:\")\n#             print(f\"  Length: {len(text)} characters\")\n#             print(f\"  Preview: {text[:200]}...\")\n#             print()\n    \n#     return sample_train, sample_valid, tokenizer\n\n# if __name__ == \"__main__\":\n#     # Note: This is a comprehensive implementation\n#     # For actual execution, you may want to run sections separately\n#     # due to computational requirements\n    \n#     print(\"GPT-2 Fine-tuning Pipeline for Next Word Prediction\")\n#     print(\"=\" * 60)\n    \n#     # Uncomment to explore training data first\n#     sample_train, sample_valid, tokenizer = explore_training_data()\n    \n#     # Uncomment the following line to run the full pipeline\n#     # model, tokenizer, results = main()\n    \n#     print(\"Pipeline setup complete!\")\n#     print(\"To explore data: uncomment explore_training_data() call\")\n#     print(\"To run training: uncomment the main() call and execute with sufficient GPU memory\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T21:35:49.079299Z","iopub.execute_input":"2025-07-08T21:35:49.079593Z","iopub.status.idle":"2025-07-08T21:35:51.933024Z","shell.execute_reply.started":"2025-07-08T21:35:49.079566Z","shell.execute_reply":"2025-07-08T21:35:51.932231Z"}},"outputs":[{"name":"stdout","text":"GPT-2 Fine-tuning Pipeline for Next Word Prediction\n============================================================\n🔍 EXPLORING TRAINING DATA\n================================================================================\nLoading sample data...\nLoading WikiText-2 dataset...\nTrain texts: 23767\nValidation texts: 2461\nTest texts: 2891\nSetting up GPT-2 model and tokenizer...\nProcessing texts...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 50/50 [00:00<00:00, 655.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Processing texts...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 20/20 [00:00<00:00, 1216.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Created sample datasets:\n  Sample train: 58 examples\n  Sample valid: 20 examples\n\n1️⃣ INSPECTING RAW TRAINING EXAMPLES\n================================================================================\nTRAINING EXAMPLES INSPECTION\n================================================================================\nDataset Info:\n  Total examples: 58\n  Max sequence length: 512\n  Tokenizer vocab size: 50257\n  Tokenizer pad token: '<|endoftext|>' (ID: 50256)\n  Tokenizer eos token: '<|endoftext|>' (ID: 50256)\n\nEXAMPLE 1:\n------------------------------------------------------------\nSequence length: 9\nAttention mask sum: 9\n\nFull text (with special tokens):\n  ' = Valkyria Chronicles III = \n'\n\nClean text (without special tokens):\n  ' = Valkyria Chronicles III = \n'\n\nFirst 20 tokens breakdown:\n  Token ID | Token Text | Attention\n  ---------|------------|----------\n       796 | ' ='       |         1\n       569 | ' V'       |         1\n     18354 | 'alky'     |         1\n      7496 | 'ria'      |         1\n     17740 | ' Chronicles' |         1\n      6711 | ' III'     |         1\n       796 | ' ='       |         1\n       220 | <SPACE>    |         1\n       198 | <SPACE>    |         1\n\n============================================================\n\nEXAMPLE 2:\n------------------------------------------------------------\nSequence length: 166\nAttention mask sum: 166\n\nFull text (with special tokens):\n  ' Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battle...'\n\nClean text (without special tokens):\n  ' Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battle...'\n\nFirst 20 tokens breakdown:\n  Token ID | Token Text | Attention\n  ---------|------------|----------\n      2311 | ' Sen'     |         1\n        73 | 'j'        |         1\n     13090 | 'ō'        |         1\n       645 | ' no'      |         1\n       569 | ' V'       |         1\n     18354 | 'alky'     |         1\n      7496 | 'ria'      |         1\n       513 | ' 3'       |         1\n      1058 | ' :'       |         1\n       791 | ' Un'      |         1\n     47398 | 'recorded' |         1\n     17740 | ' Chronicles' |         1\n       357 | ' ('       |         1\n      4960 | ' Japanese' |         1\n      1058 | ' :'       |         1\n     10545 | ' �'       |         1\n       230 | '�'        |         1\n        99 | '�'        |         1\n       161 | '�'        |         1\n       254 | '�'        |         1\n  ... (146 more tokens)\n\n============================================================\n\nEXAMPLE 3:\n------------------------------------------------------------\nSequence length: 107\nAttention mask sum: 107\n\nFull text (with special tokens):\n  ' The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chr...'\n\nClean text (without special tokens):\n  ' The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chr...'\n\nFirst 20 tokens breakdown:\n  Token ID | Token Text | Attention\n  ---------|------------|----------\n       383 | ' The'     |         1\n       983 | ' game'    |         1\n      2540 | ' began'   |         1\n      2478 | ' development' |         1\n       287 | ' in'      |         1\n      3050 | ' 2010'    |         1\n       837 | ' ,'       |         1\n      6872 | ' carrying' |         1\n       625 | ' over'    |         1\n       257 | ' a'       |         1\n      1588 | ' large'   |         1\n      6903 | ' portion' |         1\n       286 | ' of'      |         1\n       262 | ' the'     |         1\n       670 | ' work'    |         1\n      1760 | ' done'    |         1\n       319 | ' on'      |         1\n       569 | ' V'       |         1\n     18354 | 'alky'     |         1\n      7496 | 'ria'      |         1\n  ... (87 more tokens)\n\n============================================================\n\n2️⃣ NEXT WORD PREDICTION FORMAT\n================================================================================\nNEXT WORD PREDICTION TRAINING FORMAT\n================================================================================\nThis shows how each sequence is used for next word prediction training.\nFor each position, the model sees all previous tokens and predicts the next one.\n\nEXAMPLE 1:\n------------------------------------------------------------\nInput Context -> Target Token\n----------------------------------------\n' =' -> ' V'\n' = V' -> 'alky'\n' = Valky' -> 'ria'\n' = Valkyria' -> ' Chronicles'\n' = Valkyria Chronicles' -> ' III'\n' = Valkyria Chronicles III' -> ' ='\n' = Valkyria Chronicles III =' -> ' '\n' = Valkyria Chronicles III = ' -> '\n'\n\n============================================================\n\nEXAMPLE 2:\n------------------------------------------------------------\nInput Context -> Target Token\n----------------------------------------\n' Sen' -> 'j'\n' Senj' -> 'ō'\n' Senjō' -> ' no'\n' Senjō no' -> ' V'\n' Senjō no V' -> 'alky'\n' Senjō no Valky' -> 'ria'\n' Senjō no Valkyria' -> ' 3'\n' Senjō no Valkyria 3' -> ' :'\n' Senjō no Valkyria 3 :' -> ' Un'\n' Senjō no Valkyria 3 : Un' -> 'recorded'\n... (155 more prediction pairs)\n\n============================================================\n\n3️⃣ DATASET STATISTICS\n================================================================================\nDATASET STATISTICAL ANALYSIS\n================================================================================\nAnalyzing dataset... (this may take a moment)\n","output_type":"stream"},{"name":"stderr","text":"Processing examples: 100%|██████████| 58/58 [00:00<00:00, 4561.25it/s]","output_type":"stream"},{"name":"stdout","text":"Sequence Length Statistics:\n  Mean: 110.33\n  Median: 85.50\n  Min: 7\n  Max: 351\n  Std: 102.88\n\nTop 20 Most Common Tokens (from first 1000 examples):\n  Rank | Token ID | Count | Token Text\n  -----|----------|-------|----------\n     1 |      262 |   352 | ' the'\n     2 |      837 |   262 | ' ,'\n     3 |      764 |   200 | ' .'\n     4 |      286 |   151 | ' of'\n     5 |      284 |   131 | ' to'\n     6 |      290 |   108 | ' and'\n     7 |      287 |    80 | ' in'\n     8 |      257 |    79 | ' a'\n     9 |      373 |    75 | ' was'\n    10 |      569 |    59 | ' V'\n    11 |    18354 |    59 | 'alky'\n    12 |      220 |    59 | <SPACE>\n    13 |      198 |    58 | <SPACE>\n    14 |     7496 |    55 | 'ria'\n    15 |      416 |    53 | ' by'\n    16 |      383 |    52 | ' The'\n    17 |      366 |    51 | ' \"'\n    18 |      796 |    50 | ' ='\n    19 |      355 |    47 | ' as'\n    20 |      983 |    46 | ' game'\n\nTotal unique tokens in sample: 1806\n\n================================================================================\n4️⃣ SAMPLE WIKITEXT CONTENT\n========================================\nHere are some original texts from WikiText-2:\n\nText 1:\n  Length: 30 characters\n  Preview:  = Valkyria Chronicles III = \n...\n\nText 2:\n  Length: 706 characters\n  Preview:  Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ p...\n\nText 3:\n  Length: 524 characters\n  Preview:  The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adju...\n\nPipeline setup complete!\nTo explore data: uncomment explore_training_data() call\nTo run training: uncomment the main() call and execute with sufficient GPU memory\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    GPT2LMHeadModel, \n    GPT2Tokenizer, \n    TrainingArguments, \n    Trainer,\n    DataCollatorForLanguageModeling,\n    get_linear_schedule_with_warmup\n)\nfrom datasets import load_dataset\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom typing import Dict, List, Tuple\nimport math\nimport json\nimport os\n\nclass WikiTextDataset(Dataset):\n    \"\"\"Improved dataset for WikiText-2 with better tokenization and filtering\"\"\"\n    \n    def __init__(self, texts: List[str], tokenizer, max_length: int = 1024, min_length: int = 30):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.min_length = min_length\n        self.examples = []\n        \n        print(f\"Processing {len(texts)} texts...\")\n        \n        # Filter and clean texts\n        cleaned_texts = []\n        for text in texts:\n            text = text.strip()\n            if len(text) > self.min_length and not text.startswith('='):  # Skip headers\n                # Basic cleaning\n                text = text.replace('\\n\\n', ' ').replace('\\n', ' ')\n                text = ' '.join(text.split())  # Normalize whitespace\n                if len(text) > self.min_length:\n                    cleaned_texts.append(text)\n        \n        print(f\"After cleaning: {len(cleaned_texts)} texts\")\n        \n        # If no texts after cleaning, use original texts with minimal filtering\n        if len(cleaned_texts) == 0:\n            print(\"Warning: No texts after cleaning, using minimal filtering\")\n            for text in texts:\n                text = text.strip()\n                if len(text) > 10:  # Very minimal filtering\n                    cleaned_texts.append(text)\n        \n        # Tokenize with sliding window\n        for text in tqdm(cleaned_texts, desc=\"Tokenizing\"):\n            # Tokenize full text\n            tokens = tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=2048)\n            \n            # Skip very short sequences\n            if len(tokens) < 32:\n                continue\n            \n            # Create overlapping sequences with smaller stride for better coverage\n            stride = max_length // 4  # Smaller stride for more overlap\n            \n            for i in range(0, len(tokens) - max_length + 1, stride):\n                chunk = tokens[i:i + max_length]\n                if len(chunk) >= max_length:  # Ensure full length sequences\n                    self.examples.append(chunk)\n            \n            # If the text is shorter than max_length, pad it\n            if len(tokens) < max_length and len(tokens) >= 32:\n                padded_tokens = tokens + [tokenizer.pad_token_id] * (max_length - len(tokens))\n                self.examples.append(padded_tokens)\n        \n        print(f\"Created {len(self.examples)} training examples\")\n        \n        # Ensure we have at least some examples\n        if len(self.examples) == 0:\n            print(\"Warning: No examples created, creating dummy examples\")\n            # Create a few dummy examples as fallback\n            dummy_text = \"This is a sample text for training.\"\n            tokens = tokenizer.encode(dummy_text, add_special_tokens=True)\n            padded_tokens = tokens + [tokenizer.pad_token_id] * (max_length - len(tokens))\n            self.examples = [padded_tokens[:max_length] for _ in range(10)]\n    \n    def __len__(self):\n        return len(self.examples)\n    \n    def __getitem__(self, idx):\n        tokens = self.examples[idx]\n        return {\n            'input_ids': torch.tensor(tokens, dtype=torch.long),\n            'attention_mask': torch.ones(len(tokens), dtype=torch.long)\n        }\n\nclass NWPEvaluator:\n    \"\"\"Enhanced evaluator for Next Word Prediction metrics\"\"\"\n    \n    def __init__(self, model, tokenizer, device):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.device = device\n        self.model.eval()\n    \n    def calculate_perplexity(self, dataloader) -> float:\n        \"\"\"Calculate perplexity with improved loss computation\"\"\"\n        total_loss = 0\n        total_tokens = 0\n        \n        with torch.no_grad():\n            for batch in tqdm(dataloader, desc=\"Calculating perplexity\"):\n                input_ids = batch['input_ids'].to(self.device)\n                attention_mask = batch['attention_mask'].to(self.device)\n                \n                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n                loss = outputs.loss\n                \n                # Count actual tokens (exclude padding)\n                valid_tokens = attention_mask.sum().item()\n                total_loss += loss.item() * valid_tokens\n                total_tokens += valid_tokens\n        \n        avg_loss = total_loss / total_tokens\n        perplexity = math.exp(avg_loss)\n        return perplexity\n    \n    def get_next_word_probabilities(self, text: str, top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"Get top-k next word probabilities for a given text\"\"\"\n        self.model.eval()\n        \n        # Tokenize input\n        input_ids = self.tokenizer.encode(text, return_tensors='pt').to(self.device)\n        \n        with torch.no_grad():\n            outputs = self.model(input_ids)\n            logits = outputs.logits[0, -1, :]  # Get logits for next token\n            \n            # Apply softmax to get probabilities\n            probabilities = torch.softmax(logits, dim=-1)\n            \n            # Get top-k tokens and their probabilities\n            top_k_probs, top_k_indices = torch.topk(probabilities, top_k)\n            \n            # Convert to readable format\n            results = []\n            for i in range(top_k):\n                token_id = top_k_indices[i].item()\n                prob = top_k_probs[i].item()\n                token_text = self.tokenizer.decode([token_id], skip_special_tokens=True)\n                results.append((token_text, prob))\n            \n            return results\n    \n    def calculate_top_k_accuracy(self, dataloader, k_values: List[int] = [1, 5, 10]) -> Dict[int, float]:\n        \"\"\"Calculate top-k accuracy with better sampling\"\"\"\n        correct_predictions = {k: 0 for k in k_values}\n        total_predictions = 0\n        \n        with torch.no_grad():\n            for batch in tqdm(dataloader, desc=\"Calculating top-k accuracy\"):\n                input_ids = batch['input_ids'].to(self.device)\n                attention_mask = batch['attention_mask'].to(self.device)\n                \n                # Process each sequence\n                for seq_idx in range(min(input_ids.size(0), 4)):  # Limit for efficiency\n                    sequence = input_ids[seq_idx]\n                    mask = attention_mask[seq_idx]\n                    \n                    # Sample positions to evaluate (every 10th position)\n                    valid_length = mask.sum().item()\n                    positions = range(10, valid_length, 10)\n                    \n                    for pos in positions:\n                        context = sequence[:pos].unsqueeze(0)\n                        target = sequence[pos].item()\n                        \n                        outputs = self.model(context)\n                        logits = outputs.logits[0, -1, :]\n                        \n                        # Get top-k predictions\n                        top_k_max = max(k_values)\n                        top_k_tokens = torch.topk(logits, top_k_max).indices\n                        \n                        # Check accuracy for different k values\n                        for k in k_values:\n                            if target in top_k_tokens[:k]:\n                                correct_predictions[k] += 1\n                        \n                        total_predictions += 1\n        \n        # Calculate accuracies\n        accuracies = {k: correct_predictions[k] / total_predictions if total_predictions > 0 else 0 \n                     for k in k_values}\n        return accuracies\n\ndef load_and_prepare_data():\n    \"\"\"Load WikiText-2 dataset with better preprocessing\"\"\"\n    print(\"Loading WikiText-2 dataset...\")\n    dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n    \n    # Filter and clean texts\n    def clean_text(text):\n        text = text.strip()\n        # Skip short texts, headers, and empty lines\n        if len(text) < 30 or text.startswith('=') or text.startswith('@'):  # Reduced min length\n            return None\n        return text\n    \n    train_texts = [clean_text(text) for text in dataset['train']['text']]\n    valid_texts = [clean_text(text) for text in dataset['validation']['text']]\n    test_texts = [clean_text(text) for text in dataset['test']['text']]\n    \n    # Remove None values\n    train_texts = [text for text in train_texts if text is not None]\n    valid_texts = [text for text in valid_texts if text is not None]\n    test_texts = [text for text in test_texts if text is not None]\n    \n    print(f\"Train texts: {len(train_texts)}\")\n    print(f\"Validation texts: {len(valid_texts)}\")\n    print(f\"Test texts: {len(test_texts)}\")\n    \n    # Ensure we have enough data\n    if len(train_texts) == 0:\n        raise ValueError(\"No training texts found after filtering!\")\n    if len(valid_texts) == 0:\n        print(\"Warning: No validation texts found, using subset of training data\")\n        valid_texts = train_texts[:min(100, len(train_texts)//10)]\n    if len(test_texts) == 0:\n        print(\"Warning: No test texts found, using subset of training data\")\n        test_texts = train_texts[:min(50, len(train_texts)//20)]\n    \n    return train_texts, valid_texts, test_texts\n\ndef setup_model_and_tokenizer():\n    \"\"\"Initialize GPT-2 model and tokenizer\"\"\"\n    print(\"Setting up GPT-2 model and tokenizer...\")\n    model_name = \"gpt2\"  # Use base GPT-2 for better fine-tuning\n    \n    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n    model = GPT2LMHeadModel.from_pretrained(model_name)\n    \n    # Add padding token\n    tokenizer.pad_token = tokenizer.eos_token\n    \n    # Resize model embeddings if needed\n    model.resize_token_embeddings(len(tokenizer))\n    \n    return model, tokenizer\n\ndef fine_tune_model(model, tokenizer, train_dataset, eval_dataset, output_dir=\"./gpt2-wikitext2-improved\"):\n    \"\"\"Fine-tune GPT-2 with improved hyperparameters for better perplexity\"\"\"\n    \n    # Data collator\n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer,\n        mlm=False,\n        pad_to_multiple_of=8,\n        return_tensors=\"pt\"\n    )\n    \n    # Improved training arguments for better perplexity (removed early_stopping_patience)\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        overwrite_output_dir=True,\n        num_train_epochs=5,  # More epochs\n        per_device_train_batch_size=2,  # Smaller batch size\n        per_device_eval_batch_size=4,\n        gradient_accumulation_steps=4,  # Larger effective batch size\n        warmup_steps=1000,  # More warmup\n        logging_steps=50,\n        save_steps=500,\n        eval_steps=250,\n        eval_strategy=\"steps\",\n        save_strategy=\"steps\",\n        load_best_model_at_end=True,\n        metric_for_best_model=\"eval_loss\",\n        greater_is_better=False,\n        save_total_limit=3,\n        fp16=True,\n        dataloader_pin_memory=True,\n        learning_rate=3e-5,  # Lower learning rate\n        weight_decay=0.01,\n        adam_epsilon=1e-8,\n        max_grad_norm=1.0,\n        lr_scheduler_type=\"cosine\",  # Cosine scheduler\n        report_to=[],\n        remove_unused_columns=False,\n        dataloader_num_workers=4,\n    )\n    \n    # Initialize trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        data_collator=data_collator,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        tokenizer=tokenizer,\n    )\n    \n    # Train the model\n    print(\"Starting fine-tuning...\")\n    trainer.train()\n    \n    # Save the fine-tuned model\n    trainer.save_model()\n    tokenizer.save_pretrained(output_dir)\n    \n    return trainer\n\ndef evaluate_model(model, tokenizer, test_dataset, device):\n    \"\"\"Comprehensive evaluation of the fine-tuned model\"\"\"\n    print(\"Evaluating fine-tuned model...\")\n    \n    # Create test dataloader with proper collation\n    def collate_fn(batch):\n        input_ids = [item['input_ids'] for item in batch]\n        attention_masks = [item['attention_mask'] for item in batch]\n        \n        # Pad sequences\n        input_ids = torch.nn.utils.rnn.pad_sequence(\n            input_ids, batch_first=True, padding_value=tokenizer.pad_token_id\n        )\n        attention_masks = torch.nn.utils.rnn.pad_sequence(\n            attention_masks, batch_first=True, padding_value=0\n        )\n        \n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_masks\n        }\n    \n    test_dataloader = DataLoader(\n        test_dataset, \n        batch_size=4,  # Smaller batch size for evaluation\n        shuffle=False,\n        collate_fn=collate_fn\n    )\n    \n    # Initialize evaluator\n    evaluator = NWPEvaluator(model, tokenizer, device)\n    \n    # Calculate perplexity\n    perplexity = evaluator.calculate_perplexity(test_dataloader)\n    print(f\"Perplexity: {perplexity:.2f}\")\n    \n    # Calculate top-k accuracy\n    accuracies = evaluator.calculate_top_k_accuracy(test_dataloader, k_values=[1, 5, 10])\n    print(\"Top-k Accuracies:\")\n    for k, acc in accuracies.items():\n        print(f\"  Top-{k}: {acc:.4f}\")\n    \n    return perplexity, accuracies, evaluator\n\ndef save_model_for_streamlit(model, tokenizer, output_dir=\"./streamlit_model\"):\n    \"\"\"Save model and tokenizer for Streamlit app\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Save model and tokenizer\n    model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n    \n    # Save model info\n    model_info = {\n        \"model_type\": \"GPT2LMHeadModel\",\n        \"vocab_size\": len(tokenizer),\n        \"max_length\": 1024,\n        \"pad_token_id\": tokenizer.pad_token_id,\n        \"eos_token_id\": tokenizer.eos_token_id\n    }\n    \n    with open(os.path.join(output_dir, \"model_info.json\"), \"w\") as f:\n        json.dump(model_info, f, indent=2)\n    \n    print(f\"Model saved to {output_dir} for Streamlit app\")\n\ndef main():\n    \"\"\"Main training pipeline with improved settings\"\"\"\n    # Set device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    \n    # Load data\n    train_texts, valid_texts, test_texts = load_and_prepare_data()\n    \n    # Setup model and tokenizer\n    model, tokenizer = setup_model_and_tokenizer()\n    model.to(device)\n    \n    # Create datasets with improved parameters\n    print(\"Creating datasets...\")\n    # Use more data for better performance, with fallback limits\n    max_train = min(5000, len(train_texts))\n    max_valid = min(1000, len(valid_texts))\n    max_test = min(500, len(test_texts))\n    \n    train_dataset = WikiTextDataset(train_texts[:max_train], tokenizer, max_length=1024)\n    eval_dataset = WikiTextDataset(valid_texts[:max_valid], tokenizer, max_length=1024)\n    test_dataset = WikiTextDataset(test_texts[:max_test], tokenizer, max_length=1024)\n    \n    print(f\"Training examples: {len(train_dataset)}\")\n    print(f\"Validation examples: {len(eval_dataset)}\")\n    print(f\"Test examples: {len(test_dataset)}\")\n    \n    # Final check to ensure we have valid datasets\n    if len(train_dataset) == 0 or len(eval_dataset) == 0 or len(test_dataset) == 0:\n        raise ValueError(\"One or more datasets are empty after processing!\")\n    \n    # Ensure minimum dataset sizes\n    if len(train_dataset) < 10:\n        print(\"Warning: Very small training dataset!\")\n    if len(eval_dataset) < 5:\n        print(\"Warning: Very small validation dataset!\")\n    \n    # Fine-tune model\n    trainer = fine_tune_model(model, tokenizer, train_dataset, eval_dataset)\n    \n    # Evaluate model\n    perplexity, accuracies, evaluator = evaluate_model(model, tokenizer, test_dataset, device)\n    \n    # Save model for Streamlit\n    save_model_for_streamlit(model, tokenizer)\n    \n    # Create results summary\n    results = {\n        'Metric': ['Perplexity', 'Top-1 Accuracy', 'Top-5 Accuracy', 'Top-10 Accuracy'],\n        'Value': [f\"{perplexity:.2f}\", f\"{accuracies[1]:.4f}\", f\"{accuracies[5]:.4f}\", f\"{accuracies[10]:.4f}\"]\n    }\n    \n    results_df = pd.DataFrame(results)\n    print(\"\\nFinal Results Summary:\")\n    print(results_df.to_string(index=False))\n    \n    # Save results\n    results_df.to_csv(\"training_results.csv\", index=False)\n    \n    return model, tokenizer, results_df, evaluator\n\nif __name__ == \"__main__\":\n    print(\"Improved GPT-2 Fine-tuning Pipeline for Next Word Prediction\")\n    print(\"Target Perplexity: 35-40\")\n    print(\"=\" * 60)\n    \n    # Run the improved pipeline\n    model, tokenizer, results, evaluator = main()\n    \n    # Test with sample predictions\n    sample_texts = [\n        \"The president of the United States\",\n        \"Machine learning algorithms can\",\n        \"In the field of artificial intelligence\",\n        \"The weather today is\",\n        \"Scientists have discovered\"\n    ]\n    \n    print(\"\\nSample Next Word Predictions:\")\n    print(\"=\" * 50)\n    \n    for text in sample_texts:\n        predictions = evaluator.get_next_word_probabilities(text, top_k=5)\n        print(f\"\\nInput: '{text}'\")\n        print(\"Top 5 predictions:\")\n        for i, (word, prob) in enumerate(predictions, 1):\n            print(f\"  {i}. '{word}' (prob: {prob:.4f})\")\n    \n    print(\"\\nTraining complete! Model saved for Streamlit app.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T14:04:29.872735Z","iopub.execute_input":"2025-07-09T14:04:29.873056Z","iopub.status.idle":"2025-07-09T15:36:16.358642Z","shell.execute_reply.started":"2025-07-09T14:04:29.873032Z","shell.execute_reply":"2025-07-09T15:36:16.357889Z"}},"outputs":[{"name":"stdout","text":"Improved GPT-2 Fine-tuning Pipeline for Next Word Prediction\nTarget Perplexity: 35-40\n============================================================\nUsing device: cuda\nLoading WikiText-2 dataset...\nTrain texts: 16781\nValidation texts: 1766\nTest texts: 2000\nSetting up GPT-2 model and tokenizer...\nCreating datasets...\nProcessing 5000 texts...\nAfter cleaning: 4976 texts\n","output_type":"stream"},{"name":"stderr","text":"Tokenizing: 100%|██████████| 4976/4976 [00:05<00:00, 854.89it/s] \n","output_type":"stream"},{"name":"stdout","text":"Created 4366 training examples\nProcessing 1000 texts...\nAfter cleaning: 999 texts\n","output_type":"stream"},{"name":"stderr","text":"Tokenizing: 100%|██████████| 999/999 [00:01<00:00, 963.61it/s] \n","output_type":"stream"},{"name":"stdout","text":"Created 913 training examples\nProcessing 500 texts...\nAfter cleaning: 500 texts\n","output_type":"stream"},{"name":"stderr","text":"Tokenizing: 100%|██████████| 500/500 [00:00<00:00, 866.92it/s]\n/tmp/ipykernel_36/3553350375.py:293: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"name":"stdout","text":"Created 447 training examples\nTraining examples: 4366\nValidation examples: 913\nTest examples: 447\nStarting fine-tuning...\n","output_type":"stream"},{"name":"stderr","text":"`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1365' max='1365' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1365/1365 55:29, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>250</td>\n      <td>1.876900</td>\n      <td>3.564250</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.840200</td>\n      <td>3.455252</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>1.692500</td>\n      <td>3.425642</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.681500</td>\n      <td>3.419785</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>1.635000</td>\n      <td>3.418238</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nThere were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n","output_type":"stream"},{"name":"stdout","text":"Evaluating fine-tuned model...\n","output_type":"stream"},{"name":"stderr","text":"Calculating perplexity: 100%|██████████| 112/112 [00:39<00:00,  2.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity: 51575.83\n","output_type":"stream"},{"name":"stderr","text":"Calculating top-k accuracy: 100%|██████████| 112/112 [35:18<00:00, 18.92s/it]\n","output_type":"stream"},{"name":"stdout","text":"Top-k Accuracies:\n  Top-1: 0.0574\n  Top-5: 0.0872\n  Top-10: 0.0982\nModel saved to ./streamlit_model for Streamlit app\n\nFinal Results Summary:\n         Metric    Value\n     Perplexity 51575.83\n Top-1 Accuracy   0.0574\n Top-5 Accuracy   0.0872\nTop-10 Accuracy   0.0982\n\nSample Next Word Predictions:\n==================================================\n\nInput: 'The president of the United States'\nTop 5 predictions:\n  1. ' ,' (prob: 0.0901)\n  2. ' of' (prob: 0.0652)\n  3. ' Conference' (prob: 0.0366)\n  4. ' is' (prob: 0.0339)\n  5. ' has' (prob: 0.0331)\n\nInput: 'Machine learning algorithms can'\nTop 5 predictions:\n  1. ' be' (prob: 0.4474)\n  2. ' also' (prob: 0.0611)\n  3. ' help' (prob: 0.0222)\n  4. ' take' (prob: 0.0134)\n  5. ' learn' (prob: 0.0124)\n\nInput: 'In the field of artificial intelligence'\nTop 5 predictions:\n  1. ' ,' (prob: 0.8455)\n  2. ' (' (prob: 0.0561)\n  3. ' and' (prob: 0.0111)\n  4. ' research' (prob: 0.0099)\n  5. ' there' (prob: 0.0087)\n\nInput: 'The weather today is'\nTop 5 predictions:\n  1. ' generally' (prob: 0.0448)\n  2. ' mild' (prob: 0.0367)\n  3. ' very' (prob: 0.0331)\n  4. ' not' (prob: 0.0267)\n  5. ' warm' (prob: 0.0256)\n\nInput: 'Scientists have discovered'\nTop 5 predictions:\n  1. ' that' (prob: 0.3918)\n  2. ' a' (prob: 0.1420)\n  3. ' the' (prob: 0.0762)\n  4. ' an' (prob: 0.0169)\n  5. ' several' (prob: 0.0155)\n\nTraining complete! Model saved for Streamlit app.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    GPT2LMHeadModel, \n    GPT2Tokenizer, \n    TrainingArguments, \n    Trainer,\n    DataCollatorForLanguageModeling,\n    get_linear_schedule_with_warmup\n)\nfrom datasets import load_dataset\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom typing import Dict, List, Tuple\nimport math\nimport json\nimport os\n\nclass WikiTextDataset(Dataset):\n    \"\"\"Improved dataset for WikiText-2 with better tokenization and filtering\"\"\"\n    \n    def __init__(self, texts: List[str], tokenizer, max_length: int = 1024, min_length: int = 30):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.min_length = min_length\n        self.examples = []\n        \n        print(f\"Processing {len(texts)} texts...\")\n        \n        # Filter and clean texts\n        cleaned_texts = []\n        for text in texts:\n            text = text.strip()\n            if len(text) > self.min_length and not text.startswith('='):  # Skip headers\n                # Basic cleaning\n                text = text.replace('\\n\\n', ' ').replace('\\n', ' ')\n                text = ' '.join(text.split())  # Normalize whitespace\n                if len(text) > self.min_length:\n                    cleaned_texts.append(text)\n        \n        print(f\"After cleaning: {len(cleaned_texts)} texts\")\n        \n        # If no texts after cleaning, use original texts with minimal filtering\n        if len(cleaned_texts) == 0:\n            print(\"Warning: No texts after cleaning, using minimal filtering\")\n            for text in texts:\n                text = text.strip()\n                if len(text) > 10:  # Very minimal filtering\n                    cleaned_texts.append(text)\n        \n        # Tokenize with sliding window\n        for text in tqdm(cleaned_texts, desc=\"Tokenizing\"):\n            # Tokenize full text\n            tokens = tokenizer.encode(text, add_special_tokens=True, truncation=True, max_length=2048)\n            \n            # Skip very short sequences\n            if len(tokens) < 16:\n                continue\n            \n            # Create overlapping sequences with smaller stride for better coverage\n            stride = max_length // 4  # Reasonable stride\n            \n            # If text is long enough, create sliding windows\n            if len(tokens) >= max_length:\n                for i in range(0, len(tokens) - max_length + 1, stride):\n                    chunk = tokens[i:i + max_length]\n                    self.examples.append(chunk)\n            else:\n                # For shorter texts, pad to max_length\n                padded_tokens = tokens + [tokenizer.pad_token_id] * (max_length - len(tokens))\n                self.examples.append(padded_tokens)\n        \n        print(f\"Created {len(self.examples)} training examples\")\n        \n        # Ensure we have at least some examples\n        if len(self.examples) == 0:\n            print(\"Warning: No examples created, creating dummy examples\")\n            # Create a few dummy examples as fallback\n            dummy_text = \"This is a sample text for training.\"\n            tokens = tokenizer.encode(dummy_text, add_special_tokens=True)\n            padded_tokens = tokens + [tokenizer.pad_token_id] * (max_length - len(tokens))\n            self.examples = [padded_tokens[:max_length] for _ in range(10)]\n    \n    def __len__(self):\n        return len(self.examples)\n    \n    def __getitem__(self, idx):\n        tokens = self.examples[idx]\n        # Create proper attention mask (1 for real tokens, 0 for padding)\n        attention_mask = [1 if token != self.tokenizer.pad_token_id else 0 for token in tokens]\n        \n        return {\n            'input_ids': torch.tensor(tokens, dtype=torch.long),\n            'attention_mask': torch.tensor(attention_mask, dtype=torch.long)\n        }\n\nclass NWPEvaluator:\n    \"\"\"Enhanced evaluator for Next Word Prediction metrics\"\"\"\n    \n    def __init__(self, model, tokenizer, device):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.device = device\n        self.model.eval()\n    \n    def calculate_perplexity(self, dataloader) -> float:\n        \"\"\"Calculate perplexity with improved loss computation\"\"\"\n        total_loss = 0\n        total_tokens = 0\n        \n        with torch.no_grad():\n            for batch in tqdm(dataloader, desc=\"Calculating perplexity\"):\n                input_ids = batch['input_ids'].to(self.device)\n                attention_mask = batch['attention_mask'].to(self.device)\n                \n                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n                loss = outputs.loss\n                \n                # Count actual tokens (exclude padding)\n                valid_tokens = attention_mask.sum().item()\n                total_loss += loss.item() * valid_tokens\n                total_tokens += valid_tokens\n        \n        avg_loss = total_loss / total_tokens\n        perplexity = math.exp(avg_loss)\n        return perplexity\n    \n    def get_next_word_probabilities(self, text: str, top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"Get top-k next word probabilities for a given text\"\"\"\n        self.model.eval()\n        \n        # Tokenize input\n        input_ids = self.tokenizer.encode(text, return_tensors='pt').to(self.device)\n        \n        with torch.no_grad():\n            outputs = self.model(input_ids)\n            logits = outputs.logits[0, -1, :]  # Get logits for next token\n            \n            # Apply softmax to get probabilities\n            probabilities = torch.softmax(logits, dim=-1)\n            \n            # Get top-k tokens and their probabilities\n            top_k_probs, top_k_indices = torch.topk(probabilities, top_k)\n            \n            # Convert to readable format\n            results = []\n            for i in range(top_k):\n                token_id = top_k_indices[i].item()\n                prob = top_k_probs[i].item()\n                token_text = self.tokenizer.decode([token_id], skip_special_tokens=True)\n                results.append((token_text, prob))\n            \n            return results\n    \n    def calculate_top_k_accuracy(self, dataloader, k_values: List[int] = [1, 5, 10]) -> Dict[int, float]:\n        \"\"\"Calculate top-k accuracy with better sampling\"\"\"\n        correct_predictions = {k: 0 for k in k_values}\n        total_predictions = 0\n        \n        with torch.no_grad():\n            for batch in tqdm(dataloader, desc=\"Calculating top-k accuracy\"):\n                input_ids = batch['input_ids'].to(self.device)\n                attention_mask = batch['attention_mask'].to(self.device)\n                \n                # Process each sequence\n                for seq_idx in range(min(input_ids.size(0), 4)):  # Limit for efficiency\n                    sequence = input_ids[seq_idx]\n                    mask = attention_mask[seq_idx]\n                    \n                    # Sample positions to evaluate (every 10th position)\n                    valid_length = mask.sum().item()\n                    positions = range(10, valid_length, 10)\n                    \n                    for pos in positions:\n                        context = sequence[:pos].unsqueeze(0)\n                        target = sequence[pos].item()\n                        \n                        outputs = self.model(context)\n                        logits = outputs.logits[0, -1, :]\n                        \n                        # Get top-k predictions\n                        top_k_max = max(k_values)\n                        top_k_tokens = torch.topk(logits, top_k_max).indices\n                        \n                        # Check accuracy for different k values\n                        for k in k_values:\n                            if target in top_k_tokens[:k]:\n                                correct_predictions[k] += 1\n                        \n                        total_predictions += 1\n        \n        # Calculate accuracies\n        accuracies = {k: correct_predictions[k] / total_predictions if total_predictions > 0 else 0 \n                     for k in k_values}\n        return accuracies\n\ndef load_and_prepare_data():\n    \"\"\"Load WikiText-2 dataset with better preprocessing\"\"\"\n    print(\"Loading WikiText-2 dataset...\")\n    dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n    \n    # Filter and clean texts with less aggressive filtering\n    def clean_text(text):\n        text = text.strip()\n        # More lenient filtering - only skip very short texts and obvious headers\n        if len(text) < 20 or text.startswith('= =') or text.startswith(' = '):\n            return None\n        return text\n    \n    train_texts = [clean_text(text) for text in dataset['train']['text']]\n    valid_texts = [clean_text(text) for text in dataset['validation']['text']]\n    test_texts = [clean_text(text) for text in dataset['test']['text']]\n    \n    # Remove None values\n    train_texts = [text for text in train_texts if text is not None]\n    valid_texts = [text for text in valid_texts if text is not None]\n    test_texts = [text for text in test_texts if text is not None]\n    \n    print(f\"Train texts: {len(train_texts)}\")\n    print(f\"Validation texts: {len(valid_texts)}\")\n    print(f\"Test texts: {len(test_texts)}\")\n    \n    # Ensure we have enough data\n    if len(train_texts) == 0:\n        raise ValueError(\"No training texts found after filtering!\")\n    if len(valid_texts) == 0:\n        print(\"Warning: No validation texts found, using subset of training data\")\n        valid_texts = train_texts[:min(100, len(train_texts)//10)]\n    if len(test_texts) == 0:\n        print(\"Warning: No test texts found, using subset of training data\")\n        test_texts = train_texts[:min(50, len(train_texts)//20)]\n    \n    return train_texts, valid_texts, test_texts\n\ndef setup_model_and_tokenizer():\n    \"\"\"Initialize GPT-2 model and tokenizer\"\"\"\n    print(\"Setting up GPT-2 model and tokenizer...\")\n    model_name = \"gpt2\"  # Use base GPT-2 for better fine-tuning\n    \n    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n    model = GPT2LMHeadModel.from_pretrained(model_name)\n    \n    # Add padding token\n    tokenizer.pad_token = tokenizer.eos_token\n    \n    # Resize model embeddings if needed\n    model.resize_token_embeddings(len(tokenizer))\n    \n    return model, tokenizer\n\ndef fine_tune_model(model, tokenizer, train_dataset, eval_dataset, output_dir=\"./gpt2-wikitext2-improved\"):\n    \"\"\"Fine-tune GPT-2 with improved hyperparameters for better perplexity\"\"\"\n    \n    # Data collator\n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer,\n        mlm=False,\n        pad_to_multiple_of=8,\n        return_tensors=\"pt\"\n    )\n    \n    # Improved training arguments for better perplexity (fixed parameters)\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        overwrite_output_dir=True,\n        num_train_epochs=3,  # Reduced epochs for small dataset\n        per_device_train_batch_size=4,  # Larger batch size\n        per_device_eval_batch_size=8,\n        gradient_accumulation_steps=2,  # Smaller accumulation\n        warmup_steps=500,  # Reduced warmup for small dataset\n        logging_steps=50,\n        save_steps=250,\n        eval_steps=125,\n        eval_strategy=\"steps\",\n        save_strategy=\"steps\",\n        load_best_model_at_end=True,\n        metric_for_best_model=\"eval_loss\",\n        greater_is_better=False,\n        save_total_limit=2,\n        fp16=True,\n        dataloader_pin_memory=True,\n        learning_rate=5e-5,  # More conservative learning rate\n        weight_decay=0.01,\n        adam_epsilon=1e-8,\n        max_grad_norm=1.0,\n        lr_scheduler_type=\"linear\",  # Linear scheduler for stability\n        report_to=[],\n        remove_unused_columns=False,\n        dataloader_num_workers=2,\n        push_to_hub=False,\n        hub_model_id=None,\n    )\n    \n    # Initialize trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        data_collator=data_collator,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        tokenizer=tokenizer,\n    )\n    \n    # Train the model\n    print(\"Starting fine-tuning...\")\n    trainer.train()\n    \n    # Save the fine-tuned model\n    trainer.save_model()\n    tokenizer.save_pretrained(output_dir)\n    \n    return trainer\n\ndef evaluate_model(model, tokenizer, test_dataset, device):\n    \"\"\"Comprehensive evaluation of the fine-tuned model\"\"\"\n    print(\"Evaluating fine-tuned model...\")\n    \n    # Create test dataloader with proper collation\n    def collate_fn(batch):\n        input_ids = [item['input_ids'] for item in batch]\n        attention_masks = [item['attention_mask'] for item in batch]\n        \n        # Pad sequences\n        input_ids = torch.nn.utils.rnn.pad_sequence(\n            input_ids, batch_first=True, padding_value=tokenizer.pad_token_id\n        )\n        attention_masks = torch.nn.utils.rnn.pad_sequence(\n            attention_masks, batch_first=True, padding_value=0\n        )\n        \n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_masks\n        }\n    \n    test_dataloader = DataLoader(\n        test_dataset, \n        batch_size=4,  # Smaller batch size for evaluation\n        shuffle=False,\n        collate_fn=collate_fn\n    )\n    \n    # Initialize evaluator\n    evaluator = NWPEvaluator(model, tokenizer, device)\n    \n    # Calculate perplexity\n    perplexity = evaluator.calculate_perplexity(test_dataloader)\n    print(f\"Perplexity: {perplexity:.2f}\")\n    \n    # Calculate top-k accuracy\n    accuracies = evaluator.calculate_top_k_accuracy(test_dataloader, k_values=[1, 5, 10])\n    print(\"Top-k Accuracies:\")\n    for k, acc in accuracies.items():\n        print(f\"  Top-{k}: {acc:.4f}\")\n    \n    return perplexity, accuracies, evaluator\n\ndef save_model_for_streamlit(model, tokenizer, output_dir=\"./streamlit_model\"):\n    \"\"\"Save model and tokenizer for Streamlit app\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Save model and tokenizer\n    model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n    \n    # Save model info\n    model_info = {\n        \"model_type\": \"GPT2LMHeadModel\",\n        \"vocab_size\": len(tokenizer),\n        \"max_length\": 1024,\n        \"pad_token_id\": tokenizer.pad_token_id,\n        \"eos_token_id\": tokenizer.eos_token_id\n    }\n    \n    with open(os.path.join(output_dir, \"model_info.json\"), \"w\") as f:\n        json.dump(model_info, f, indent=2)\n    \n    print(f\"Model saved to {output_dir} for Streamlit app\")\n\ndef main():\n    \"\"\"Main training pipeline with improved settings\"\"\"\n    # Set device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    \n    # Load data\n    train_texts, valid_texts, test_texts = load_and_prepare_data()\n    \n    # Setup model and tokenizer\n    model, tokenizer = setup_model_and_tokenizer()\n    model.to(device)\n    \n    # Create datasets with improved parameters\n    print(\"Creating datasets...\")\n    # Use ALL available data to maximize training examples\n    train_dataset = WikiTextDataset(train_texts, tokenizer, max_length=512)  # Shorter sequences\n    eval_dataset = WikiTextDataset(valid_texts, tokenizer, max_length=512)\n    test_dataset = WikiTextDataset(test_texts, tokenizer, max_length=512)\n    \n    print(f\"Training examples: {len(train_dataset)}\")\n    print(f\"Validation examples: {len(eval_dataset)}\")\n    print(f\"Test examples: {len(test_dataset)}\")\n    \n    # Final check to ensure we have valid datasets\n    if len(train_dataset) == 0 or len(eval_dataset) == 0 or len(test_dataset) == 0:\n        raise ValueError(\"One or more datasets are empty after processing!\")\n    \n    # Ensure minimum dataset sizes\n    if len(train_dataset) < 100:\n        print(\"Warning: Very small training dataset! Consider using more data or less aggressive filtering.\")\n    if len(eval_dataset) < 10:\n        print(\"Warning: Very small validation dataset!\")\n    \n    # Fine-tune model\n    trainer = fine_tune_model(model, tokenizer, train_dataset, eval_dataset)\n    \n    # Evaluate model\n    perplexity, accuracies, evaluator = evaluate_model(model, tokenizer, test_dataset, device)\n    \n    # Save model for Streamlit\n    save_model_for_streamlit(model, tokenizer)\n    \n    # Create results summary\n    results = {\n        'Metric': ['Perplexity', 'Top-1 Accuracy', 'Top-5 Accuracy', 'Top-10 Accuracy'],\n        'Value': [f\"{perplexity:.2f}\", f\"{accuracies[1]:.4f}\", f\"{accuracies[5]:.4f}\", f\"{accuracies[10]:.4f}\"]\n    }\n    \n    results_df = pd.DataFrame(results)\n    print(\"\\nFinal Results Summary:\")\n    print(results_df.to_string(index=False))\n    \n    # Save results\n    results_df.to_csv(\"training_results.csv\", index=False)\n    \n    return model, tokenizer, results_df, evaluator\n\nif __name__ == \"__main__\":\n    print(\"Improved GPT-2 Fine-tuning Pipeline for Next Word Prediction\")\n    print(\"Target Perplexity: 35-40\")\n    print(\"=\" * 60)\n    \n    # Run the improved pipeline\n    model, tokenizer, results, evaluator = main()\n    \n    # Test with sample predictions\n    sample_texts = [\n        \"The president of the United States\",\n        \"Machine learning algorithms can\",\n        \"In the field of artificial intelligence\",\n        \"The weather today is\",\n        \"Scientists have discovered\"\n    ]\n    \n    print(\"\\nSample Next Word Predictions:\")\n    print(\"=\" * 50)\n    \n    for text in sample_texts:\n        predictions = evaluator.get_next_word_probabilities(text, top_k=5)\n        print(f\"\\nInput: '{text}'\")\n        print(\"Top 5 predictions:\")\n        for i, (word, prob) in enumerate(predictions, 1):\n            print(f\"  {i}. '{word}' (prob: {prob:.4f})\")\n    \n    print(\"\\nTraining complete! Model saved for Streamlit app.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T15:41:46.828159Z","iopub.execute_input":"2025-07-09T15:41:46.828492Z","iopub.status.idle":"2025-07-09T17:07:26.916477Z","shell.execute_reply.started":"2025-07-09T15:41:46.828466Z","shell.execute_reply":"2025-07-09T17:07:26.915418Z"}},"outputs":[{"name":"stdout","text":"Improved GPT-2 Fine-tuning Pipeline for Next Word Prediction\nTarget Perplexity: 35-40\n============================================================\nUsing device: cuda\nLoading WikiText-2 dataset...\nTrain texts: 17537\nValidation texts: 1850\nTest texts: 2114\nSetting up GPT-2 model and tokenizer...\nCreating datasets...\nProcessing 17537 texts...\nAfter cleaning: 16736 texts\n","output_type":"stream"},{"name":"stderr","text":"Tokenizing: 100%|██████████| 16736/16736 [00:18<00:00, 900.52it/s] \n","output_type":"stream"},{"name":"stdout","text":"Created 15868 training examples\nProcessing 1850 texts...\nAfter cleaning: 1762 texts\n","output_type":"stream"},{"name":"stderr","text":"Tokenizing: 100%|██████████| 1762/1762 [00:01<00:00, 1027.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Created 1686 training examples\nProcessing 2114 texts...\nAfter cleaning: 1995 texts\n","output_type":"stream"},{"name":"stderr","text":"Tokenizing: 100%|██████████| 1995/1995 [00:01<00:00, 1000.02it/s]\n/tmp/ipykernel_36/3102765324.py:298: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"name":"stdout","text":"Created 1892 training examples\nTraining examples: 15868\nValidation examples: 1686\nTest examples: 1892\nStarting fine-tuning...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2976' max='2976' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2976/2976 1:17:59, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>125</td>\n      <td>1.920600</td>\n      <td>3.637423</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.796200</td>\n      <td>3.521796</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>1.810400</td>\n      <td>3.480217</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.778800</td>\n      <td>3.459075</td>\n    </tr>\n    <tr>\n      <td>625</td>\n      <td>1.661200</td>\n      <td>3.446401</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>1.735000</td>\n      <td>3.438861</td>\n    </tr>\n    <tr>\n      <td>875</td>\n      <td>1.686200</td>\n      <td>3.428798</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.756700</td>\n      <td>3.418926</td>\n    </tr>\n    <tr>\n      <td>1125</td>\n      <td>1.588200</td>\n      <td>3.418006</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>1.702400</td>\n      <td>3.409365</td>\n    </tr>\n    <tr>\n      <td>1375</td>\n      <td>1.649100</td>\n      <td>3.406263</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.684000</td>\n      <td>3.398934</td>\n    </tr>\n    <tr>\n      <td>1625</td>\n      <td>1.666700</td>\n      <td>3.395702</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>1.698100</td>\n      <td>3.389011</td>\n    </tr>\n    <tr>\n      <td>1875</td>\n      <td>1.663100</td>\n      <td>3.389205</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.668200</td>\n      <td>3.388435</td>\n    </tr>\n    <tr>\n      <td>2125</td>\n      <td>1.581300</td>\n      <td>3.389648</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>1.615300</td>\n      <td>3.386597</td>\n    </tr>\n    <tr>\n      <td>2375</td>\n      <td>1.594600</td>\n      <td>3.384153</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>1.598300</td>\n      <td>3.384620</td>\n    </tr>\n    <tr>\n      <td>2625</td>\n      <td>1.637200</td>\n      <td>3.381178</td>\n    </tr>\n    <tr>\n      <td>2750</td>\n      <td>1.603900</td>\n      <td>3.379966</td>\n    </tr>\n    <tr>\n      <td>2875</td>\n      <td>1.564200</td>\n      <td>3.378859</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nThere were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n","output_type":"stream"},{"name":"stdout","text":"Evaluating fine-tuned model...\n","output_type":"stream"},{"name":"stderr","text":"Calculating perplexity: 100%|██████████| 473/473 [01:28<00:00,  5.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity: 4275.16\n","output_type":"stream"},{"name":"stderr","text":"Calculating top-k accuracy: 100%|██████████| 473/473 [05:41<00:00,  1.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Top-k Accuracies:\n  Top-1: 0.3996\n  Top-5: 0.6193\n  Top-10: 0.6925\nModel saved to ./streamlit_model for Streamlit app\n\nFinal Results Summary:\n         Metric   Value\n     Perplexity 4275.16\n Top-1 Accuracy  0.3996\n Top-5 Accuracy  0.6193\nTop-10 Accuracy  0.6925\n\nSample Next Word Predictions:\n==================================================\n\nInput: 'The president of the United States'\nTop 5 predictions:\n  1. ' ,' (prob: 0.1766)\n  2. ' was' (prob: 0.0511)\n  3. ' is' (prob: 0.0421)\n  4. ' of' (prob: 0.0377)\n  5. ' has' (prob: 0.0341)\n\nInput: 'Machine learning algorithms can'\nTop 5 predictions:\n  1. ' be' (prob: 0.4724)\n  2. ' also' (prob: 0.0639)\n  3. ' perform' (prob: 0.0197)\n  4. ' learn' (prob: 0.0191)\n  5. ' help' (prob: 0.0172)\n\nInput: 'In the field of artificial intelligence'\nTop 5 predictions:\n  1. ' ,' (prob: 0.8406)\n  2. ' (' (prob: 0.0638)\n  3. ' and' (prob: 0.0150)\n  4. ' research' (prob: 0.0105)\n  5. ' there' (prob: 0.0084)\n\nInput: 'The weather today is'\nTop 5 predictions:\n  1. ' generally' (prob: 0.1253)\n  2. ' mild' (prob: 0.0389)\n  3. ' very' (prob: 0.0378)\n  4. ' warm' (prob: 0.0350)\n  5. ' mostly' (prob: 0.0315)\n\nInput: 'Scientists have discovered'\nTop 5 predictions:\n  1. ' that' (prob: 0.2698)\n  2. ' a' (prob: 0.1348)\n  3. ' the' (prob: 0.0817)\n  4. ' several' (prob: 0.0251)\n  5. ' many' (prob: 0.0244)\n\nTraining complete! Model saved for Streamlit app.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    GPT2LMHeadModel, \n    GPT2Tokenizer, \n    TrainingArguments, \n    Trainer,\n    DataCollatorForLanguageModeling,\n    get_linear_schedule_with_warmup\n)\nfrom datasets import load_dataset\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom typing import Dict, List, Tuple\nimport math\nimport json\nimport os\nimport warnings\n\n# Suppress common warnings\nwarnings.filterwarnings(\"ignore\", message=\"Was asked to gather along dimension 0\")\n\nclass WikiTextDataset(Dataset):\n    \"\"\"Fixed dataset with proper tokenization and no sliding window issues\"\"\"\n    \n    def __init__(self, texts: List[str], tokenizer, max_length: int = 512, min_length: int = 50):\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.examples = []\n        \n        print(f\"Processing {len(texts)} texts...\")\n        \n        # More conservative text filtering\n        valid_texts = []\n        for text in texts:\n            text = text.strip()\n            # Skip empty texts, headers, and very short texts\n            if (len(text) > min_length and \n                not text.startswith('=') and \n                not text.startswith(' =') and\n                len(text.split()) > 10):  # At least 10 words\n                # Basic cleaning\n                text = ' '.join(text.split())  # Normalize whitespace\n                valid_texts.append(text)\n        \n        print(f\"Valid texts after filtering: {len(valid_texts)}\")\n        \n        # Concatenate texts with separator instead of sliding window\n        if valid_texts:\n            # Join texts with EOS token\n            combined_text = f\" {tokenizer.eos_token} \".join(valid_texts)\n            \n            # Tokenize the combined text\n            tokens = tokenizer.encode(combined_text, add_special_tokens=False)\n            \n            # Split into chunks of max_length\n            for i in range(0, len(tokens), max_length):\n                chunk = tokens[i:i + max_length]\n                if len(chunk) >= 32:  # Minimum chunk size\n                    # Pad if necessary\n                    if len(chunk) < max_length:\n                        chunk = chunk + [tokenizer.pad_token_id] * (max_length - len(chunk))\n                    self.examples.append(chunk)\n        \n        print(f\"Created {len(self.examples)} training examples\")\n        \n        # Fallback if no examples created\n        if len(self.examples) == 0:\n            print(\"ERROR: No examples created! Using fallback...\")\n            # Create meaningful fallback examples\n            fallback_texts = [\n                \"This is a sample text for language modeling training.\",\n                \"Natural language processing is a field of artificial intelligence.\",\n                \"Machine learning models learn patterns from data.\",\n                \"Deep learning uses neural networks with multiple layers.\",\n                \"The quick brown fox jumps over the lazy dog.\",\n                \"Artificial intelligence has revolutionized many industries.\",\n                \"Computer vision enables machines to understand images.\",\n                \"Speech recognition converts audio to text automatically.\",\n                \"Neural networks are inspired by biological brain structures.\",\n                \"Data science combines statistics with programming skills.\"\n            ]\n            \n            for text in fallback_texts:\n                tokens = tokenizer.encode(text, add_special_tokens=True)\n                if len(tokens) < max_length:\n                    tokens = tokens + [tokenizer.pad_token_id] * (max_length - len(tokens))\n                else:\n                    tokens = tokens[:max_length]\n                self.examples.append(tokens)\n    \n    def __len__(self):\n        return len(self.examples)\n    \n    def __getitem__(self, idx):\n        tokens = self.examples[idx]\n        # Ensure we have proper tensors\n        input_ids = torch.tensor(tokens, dtype=torch.long)\n        \n        # Create attention mask (1 for real tokens, 0 for padding)\n        attention_mask = torch.tensor(\n            [1 if token != self.tokenizer.pad_token_id else 0 for token in tokens], \n            dtype=torch.long\n        )\n        \n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'labels': input_ids.clone()  # For language modeling, labels = input_ids\n        }\n\n\nclass NWPEvaluator:\n    \"\"\"Fixed evaluator with proper perplexity calculation\"\"\"\n    \n    def __init__(self, model, tokenizer, device):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.device = device\n        self.model.eval()\n    \n    def calculate_perplexity(self, dataloader) -> float:\n        \"\"\"Fixed perplexity calculation that properly handles padding\"\"\"\n        total_loss = 0\n        total_tokens = 0\n        \n        with torch.no_grad():\n            for batch in tqdm(dataloader, desc=\"Calculating perplexity\"):\n                input_ids = batch['input_ids'].to(self.device)\n                attention_mask = batch['attention_mask'].to(self.device)\n                labels = batch['labels'].to(self.device)\n                \n                # Forward pass\n                outputs = self.model(\n                    input_ids=input_ids, \n                    attention_mask=attention_mask, \n                    labels=labels\n                )\n                \n                # Get loss and logits\n                loss = outputs.loss\n                logits = outputs.logits\n                \n                # Calculate loss manually to ensure proper handling\n                shift_logits = logits[..., :-1, :].contiguous()\n                shift_labels = labels[..., 1:].contiguous()\n                shift_attention_mask = attention_mask[..., 1:].contiguous()\n                \n                # Flatten for loss calculation\n                shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n                shift_labels = shift_labels.view(-1)\n                shift_attention_mask = shift_attention_mask.view(-1)\n                \n                # Calculate loss only for non-padded tokens\n                loss_fct = nn.CrossEntropyLoss(reduction='none')\n                losses = loss_fct(shift_logits, shift_labels)\n                \n                # Mask out padded tokens\n                masked_losses = losses * shift_attention_mask.float()\n                \n                # Sum up losses and token counts\n                total_loss += masked_losses.sum().item()\n                total_tokens += shift_attention_mask.sum().item()\n        \n        if total_tokens == 0:\n            return float('inf')\n        \n        avg_loss = total_loss / total_tokens\n        perplexity = math.exp(avg_loss)\n        \n        # Cap perplexity at reasonable value to detect issues\n        if perplexity > 1000:\n            print(f\"WARNING: Very high perplexity {perplexity:.2f} detected!\")\n            print(f\"Average loss: {avg_loss:.4f}\")\n            print(f\"Total tokens: {total_tokens}\")\n        \n        return perplexity\n    \n    def get_next_word_probabilities(self, text: str, top_k: int = 10) -> List[Tuple[str, float]]:\n        \"\"\"Get top-k next word probabilities for a given text\"\"\"\n        self.model.eval()\n        \n        # Tokenize input\n        input_ids = self.tokenizer.encode(text, return_tensors='pt').to(self.device)\n        \n        with torch.no_grad():\n            outputs = self.model(input_ids)\n            logits = outputs.logits[0, -1, :]  # Get logits for next token\n            \n            # Apply softmax to get probabilities\n            probabilities = torch.softmax(logits, dim=-1)\n            \n            # Get top-k tokens and their probabilities\n            top_k_probs, top_k_indices = torch.topk(probabilities, top_k)\n            \n            # Convert to readable format\n            results = []\n            for i in range(top_k):\n                token_id = top_k_indices[i].item()\n                prob = top_k_probs[i].item()\n                token_text = self.tokenizer.decode([token_id], skip_special_tokens=True)\n                results.append((token_text, prob))\n            \n            return results\n    \n    def calculate_top_k_accuracy(self, dataloader, k_values: List[int] = [1, 5, 10]) -> Dict[int, float]:\n        \"\"\"Calculate top-k accuracy with better sampling\"\"\"\n        correct_predictions = {k: 0 for k in k_values}\n        total_predictions = 0\n        \n        with torch.no_grad():\n            for batch in tqdm(dataloader, desc=\"Calculating top-k accuracy\"):\n                input_ids = batch['input_ids'].to(self.device)\n                attention_mask = batch['attention_mask'].to(self.device)\n                \n                # Process each sequence\n                for seq_idx in range(min(input_ids.size(0), 2)):  # Limit for efficiency\n                    sequence = input_ids[seq_idx]\n                    mask = attention_mask[seq_idx]\n                    \n                    # Sample positions to evaluate (every 20th position)\n                    valid_length = mask.sum().item()\n                    positions = range(20, valid_length - 1, 20)\n                    \n                    for pos in positions:\n                        if pos >= valid_length - 1:\n                            continue\n                            \n                        context = sequence[:pos].unsqueeze(0)\n                        target = sequence[pos].item()\n                        \n                        outputs = self.model(context)\n                        logits = outputs.logits[0, -1, :]\n                        \n                        # Get top-k predictions\n                        top_k_max = max(k_values)\n                        top_k_tokens = torch.topk(logits, top_k_max).indices\n                        \n                        # Check accuracy for different k values\n                        for k in k_values:\n                            if target in top_k_tokens[:k]:\n                                correct_predictions[k] += 1\n                        \n                        total_predictions += 1\n        \n        # Calculate accuracies\n        accuracies = {k: correct_predictions[k] / total_predictions if total_predictions > 0 else 0 \n                     for k in k_values}\n        return accuracies\n\n\ndef debug_dataset(dataset, tokenizer, num_samples=3):\n    \"\"\"Debug function to examine dataset contents\"\"\"\n    print(f\"\\nDEBUG: Dataset has {len(dataset)} examples\")\n    \n    for i in range(min(num_samples, len(dataset))):\n        example = dataset[i]\n        input_ids = example['input_ids']\n        attention_mask = example['attention_mask']\n        \n        print(f\"\\nExample {i+1}:\")\n        print(f\"Input IDs shape: {input_ids.shape}\")\n        print(f\"Attention mask shape: {attention_mask.shape}\")\n        print(f\"Non-padding tokens: {attention_mask.sum().item()}\")\n        \n        # Decode first 50 tokens\n        decoded = tokenizer.decode(input_ids[:50], skip_special_tokens=False)\n        print(f\"First 50 tokens: {decoded}\")\n        \n        # Check for issues\n        if attention_mask.sum().item() < 10:\n            print(\"WARNING: Very few non-padding tokens!\")\n        if input_ids.max().item() >= len(tokenizer):\n            print(\"WARNING: Token ID out of vocabulary range!\")\n\n\ndef load_and_prepare_data():\n    \"\"\"Load WikiText-2 dataset with better preprocessing\"\"\"\n    print(\"Loading WikiText-2 dataset...\")\n    try:\n        dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')\n    except Exception as e:\n        print(f\"Error loading dataset: {e}\")\n        print(\"Using fallback data...\")\n        # Create fallback dataset\n        fallback_train = [\n            \"Machine learning is a subset of artificial intelligence that focuses on algorithms.\",\n            \"Deep learning uses neural networks with multiple layers to process data.\",\n            \"Natural language processing enables computers to understand human language.\",\n            \"Computer vision allows machines to interpret and analyze visual information.\",\n            \"Data science combines statistics, programming, and domain expertise.\",\n            \"Artificial intelligence has applications in healthcare, finance, and transportation.\",\n            \"Neural networks are inspired by the structure of biological brains.\",\n            \"Big data refers to large volumes of structured and unstructured data.\",\n            \"Cloud computing provides scalable and flexible IT resources over the internet.\",\n            \"Cybersecurity protects digital systems from threats and attacks.\"\n        ] * 20  # Repeat to create more training data\n        \n        fallback_valid = fallback_train[:50]\n        fallback_test = fallback_train[:25]\n        \n        return fallback_train, fallback_valid, fallback_test\n    \n    # Filter and clean texts with less aggressive filtering\n    def clean_text(text):\n        text = text.strip()\n        # More lenient filtering - only skip very short texts and obvious headers\n        if len(text) < 30 or text.startswith('= =') or text.startswith(' = '):\n            return None\n        return text\n    \n    train_texts = [clean_text(text) for text in dataset['train']['text']]\n    valid_texts = [clean_text(text) for text in dataset['validation']['text']]\n    test_texts = [clean_text(text) for text in dataset['test']['text']]\n    \n    # Remove None values\n    train_texts = [text for text in train_texts if text is not None]\n    valid_texts = [text for text in valid_texts if text is not None]\n    test_texts = [text for text in test_texts if text is not None]\n    \n    print(f\"Train texts: {len(train_texts)}\")\n    print(f\"Validation texts: {len(valid_texts)}\")\n    print(f\"Test texts: {len(test_texts)}\")\n    \n    # Ensure we have enough data\n    if len(train_texts) == 0:\n        print(\"Warning: No training texts found, using fallback\")\n        train_texts = [\"This is sample training text for language modeling.\"] * 100\n    if len(valid_texts) == 0:\n        print(\"Warning: No validation texts found, using subset of training data\")\n        valid_texts = train_texts[:min(50, len(train_texts)//10)]\n    if len(test_texts) == 0:\n        print(\"Warning: No test texts found, using subset of training data\")\n        test_texts = train_texts[:min(25, len(train_texts)//20)]\n    \n    return train_texts, valid_texts, test_texts\n\n\ndef setup_model_and_tokenizer():\n    \"\"\"Initialize GPT-2 model and tokenizer\"\"\"\n    print(\"Setting up GPT-2 model and tokenizer...\")\n    model_name = \"gpt2\"  # Use base GPT-2 for better fine-tuning\n    \n    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n    model = GPT2LMHeadModel.from_pretrained(model_name)\n    \n    # Add padding token\n    tokenizer.pad_token = tokenizer.eos_token\n    \n    # Resize model embeddings if needed\n    model.resize_token_embeddings(len(tokenizer))\n    \n    return model, tokenizer\n\n\nclass CustomDataCollator:\n    \"\"\"Custom data collator to ensure proper batch formatting\"\"\"\n    \n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n    \n    def __call__(self, features):\n        # Extract components\n        input_ids = [f['input_ids'] for f in features]\n        attention_masks = [f['attention_mask'] for f in features]\n        labels = [f['labels'] for f in features]\n        \n        # Stack tensors\n        batch = {\n            'input_ids': torch.stack(input_ids),\n            'attention_mask': torch.stack(attention_masks),\n            'labels': torch.stack(labels)\n        }\n        \n        return batch\n\n\ndef fine_tune_model(model, tokenizer, train_dataset, eval_dataset, output_dir=\"./gpt2-wikitext2-fixed\"):\n    \"\"\"Fine-tune GPT-2 with improved hyperparameters\"\"\"\n    \n    # Custom data collator\n    data_collator = CustomDataCollator(tokenizer)\n    \n    # Conservative training arguments\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        overwrite_output_dir=True,\n        num_train_epochs=3,  # Moderate number of epochs\n        per_device_train_batch_size=2,  # Small batch size\n        per_device_eval_batch_size=4,\n        gradient_accumulation_steps=4,  # Increased accumulation\n        warmup_steps=100,  # Reduced warmup\n        logging_steps=25,\n        save_steps=500,\n        eval_steps=250,\n        eval_strategy=\"steps\",\n        save_strategy=\"steps\",\n        load_best_model_at_end=True,\n        metric_for_best_model=\"eval_loss\",\n        greater_is_better=False,\n        save_total_limit=2,\n        fp16=False,  # Disable fp16 for stability\n        dataloader_pin_memory=True,\n        learning_rate=1e-5,  # Very conservative learning rate\n        weight_decay=0.01,\n        adam_epsilon=1e-8,\n        max_grad_norm=1.0,\n        lr_scheduler_type=\"cosine\",\n        report_to=[],\n        remove_unused_columns=False,\n        dataloader_num_workers=0,  # Disable multiprocessing for debugging\n        push_to_hub=False,\n        hub_model_id=None,\n        logging_dir=\"./logs\",\n        logging_strategy=\"steps\",\n        logging_first_step=True,\n        prediction_loss_only=True,\n    )\n    \n    # Initialize trainer\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        data_collator=data_collator,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        tokenizer=tokenizer,\n    )\n    \n    # Train the model\n    print(\"Starting fine-tuning...\")\n    trainer.train()\n    \n    # Save the fine-tuned model\n    trainer.save_model()\n    tokenizer.save_pretrained(output_dir)\n    \n    return trainer\n\n\ndef evaluate_model(model, tokenizer, test_dataset, device):\n    \"\"\"Comprehensive evaluation of the fine-tuned model\"\"\"\n    print(\"Evaluating fine-tuned model...\")\n    \n    # Create test dataloader\n    data_collator = CustomDataCollator(tokenizer)\n    test_dataloader = DataLoader(\n        test_dataset, \n        batch_size=2,  # Small batch size for evaluation\n        shuffle=False,\n        collate_fn=data_collator\n    )\n    \n    # Initialize evaluator\n    evaluator = NWPEvaluator(model, tokenizer, device)\n    \n    # Calculate perplexity\n    perplexity = evaluator.calculate_perplexity(test_dataloader)\n    print(f\"Perplexity: {perplexity:.2f}\")\n    \n    # Calculate top-k accuracy\n    accuracies = evaluator.calculate_top_k_accuracy(test_dataloader, k_values=[1, 5, 10])\n    print(\"Top-k Accuracies:\")\n    for k, acc in accuracies.items():\n        print(f\"  Top-{k}: {acc:.4f}\")\n    \n    return perplexity, accuracies, evaluator\n\n\ndef save_model_for_streamlit(model, tokenizer, output_dir=\"./streamlit_model\"):\n    \"\"\"Save model and tokenizer for Streamlit app\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Save model and tokenizer\n    model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n    \n    # Save model info\n    model_info = {\n        \"model_type\": \"GPT2LMHeadModel\",\n        \"vocab_size\": len(tokenizer),\n        \"max_length\": 512,\n        \"pad_token_id\": tokenizer.pad_token_id,\n        \"eos_token_id\": tokenizer.eos_token_id\n    }\n    \n    with open(os.path.join(output_dir, \"model_info.json\"), \"w\") as f:\n        json.dump(model_info, f, indent=2)\n    \n    print(f\"Model saved to {output_dir} for Streamlit app\")\n\n\ndef main():\n    \"\"\"Main training pipeline with comprehensive fixes\"\"\"\n    # Set device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    \n    # Load data\n    train_texts, valid_texts, test_texts = load_and_prepare_data()\n    \n    # Debug raw data\n    print(\"\\nSample training texts:\")\n    for i, text in enumerate(train_texts[:3]):\n        print(f\"{i+1}: {text[:200]}...\")\n    \n    # Setup model and tokenizer\n    model, tokenizer = setup_model_and_tokenizer()\n    model.to(device)\n    \n    # Create datasets\n    print(\"Creating datasets...\")\n    train_dataset = WikiTextDataset(train_texts, tokenizer, max_length=512)\n    eval_dataset = WikiTextDataset(valid_texts, tokenizer, max_length=512)\n    test_dataset = WikiTextDataset(test_texts, tokenizer, max_length=512)\n    \n    print(f\"Training examples: {len(train_dataset)}\")\n    print(f\"Validation examples: {len(eval_dataset)}\")\n    print(f\"Test examples: {len(test_dataset)}\")\n    \n    # Debug datasets\n    debug_dataset(train_dataset, tokenizer, num_samples=2)\n    debug_dataset(eval_dataset, tokenizer, num_samples=1)\n    \n    # Ensure we have valid datasets\n    if len(train_dataset) == 0 or len(eval_dataset) == 0 or len(test_dataset) == 0:\n        raise ValueError(\"One or more datasets are empty after processing!\")\n    \n    # Fine-tune model\n    trainer = fine_tune_model(model, tokenizer, train_dataset, eval_dataset)\n    \n    # Evaluate model\n    perplexity, accuracies, evaluator = evaluate_model(model, tokenizer, test_dataset, device)\n    \n    # Save model for Streamlit\n    save_model_for_streamlit(model, tokenizer)\n    \n    # Create results summary\n    results = {\n        'Metric': ['Perplexity', 'Top-1 Accuracy', 'Top-5 Accuracy', 'Top-10 Accuracy'],\n        'Value': [f\"{perplexity:.2f}\", f\"{accuracies[1]:.4f}\", f\"{accuracies[5]:.4f}\", f\"{accuracies[10]:.4f}\"]\n    }\n    \n    results_df = pd.DataFrame(results)\n    print(\"\\nFinal Results Summary:\")\n    print(results_df.to_string(index=False))\n    \n    # Save results\n    results_df.to_csv(\"training_results.csv\", index=False)\n    \n    return model, tokenizer, results_df, evaluator\n\n\nif __name__ == \"__main__\":\n    print(\"Fixed GPT-2 Fine-tuning Pipeline for Next Word Prediction\")\n    print(\"Target Perplexity: 35-40\")\n    print(\"=\" * 60)\n    \n    try:\n        # Run the fixed pipeline\n        model, tokenizer, results, evaluator = main()\n        \n        # Test with sample predictions\n        sample_texts = [\n            \"The president of the United States\",\n            \"Machine learning algorithms can\",\n            \"In the field of artificial intelligence\",\n            \"The weather today is\",\n            \"Scientists have discovered\"\n        ]\n        \n        print(\"\\nSample Next Word Predictions:\")\n        print(\"=\" * 50)\n        \n        for text in sample_texts:\n            try:\n                predictions = evaluator.get_next_word_probabilities(text, top_k=5)\n                print(f\"\\nInput: '{text}'\")\n                print(\"Top 5 predictions:\")\n                for i, (word, prob) in enumerate(predictions, 1):\n                    print(f\"  {i}. '{word}' (prob: {prob:.4f})\")\n            except Exception as e:\n                print(f\"Error with text '{text}': {e}\")\n        \n        print(\"\\nTraining complete! Model saved for Streamlit app.\")\n        \n    except Exception as e:\n        print(f\"Error during training: {e}\")\n        import traceback\n        traceback.print_exc()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:20:01.571689Z","iopub.execute_input":"2025-07-09T17:20:01.572438Z","iopub.status.idle":"2025-07-09T17:45:52.855981Z","shell.execute_reply.started":"2025-07-09T17:20:01.572411Z","shell.execute_reply":"2025-07-09T17:45:52.855240Z"}},"outputs":[{"name":"stdout","text":"Fixed GPT-2 Fine-tuning Pipeline for Next Word Prediction\nTarget Perplexity: 35-40\n============================================================\nUsing device: cuda\nLoading WikiText-2 dataset...\nTrain texts: 16965\nValidation texts: 1784\nTest texts: 2017\n\nSample training texts:\n1: Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ pl...\n2: The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjus...\n3: It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year ...\nSetting up GPT-2 model and tokenizer...\nCreating datasets...\nProcessing 16965 texts...\nValid texts after filtering: 15942\n","output_type":"stream"},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (2314275 > 1024). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"Created 4521 training examples\nProcessing 1784 texts...\nValid texts after filtering: 1708\nCreated 469 training examples\nProcessing 2017 texts...\nValid texts after filtering: 1907\nCreated 535 training examples\nTraining examples: 4521\nValidation examples: 469\nTest examples: 535\n\nDEBUG: Dataset has 4521 examples\n\nExample 1:\nInput IDs shape: torch.Size([512])\nAttention mask shape: torch.Size([512])\nNon-padding tokens: 509\nFirst 50 tokens: Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III\n\nExample 2:\nInput IDs shape: torch.Size([512])\nAttention mask shape: torch.Size([512])\nNon-padding tokens: 510\nFirst 50 tokens:  is sealed off to the player . Outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . Alongside the main story missions are character @-@ specific sub missions relating to different squad members . After the\n\nDEBUG: Dataset has 469 examples\n\nExample 1:\nInput IDs shape: torch.Size([512])\nAttention mask shape: torch.Size([512])\nNon-padding tokens: 508\nFirst 50 tokens: Homarus gammarus , known as the European lobster or common lobster , is a species of clawed lobster from the eastern Atlantic Ocean , Mediterranean Sea and parts of the Black Sea . It is closely related to the American lobster , H. americanus\nStarting fine-tuning...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/1794405346.py:424: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='849' max='849' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [849/849 19:20, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>250</td>\n      <td>1.728600</td>\n      <td>3.304678</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.691300</td>\n      <td>3.263170</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>1.690500</td>\n      <td>3.254225</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n","output_type":"stream"},{"name":"stdout","text":"Evaluating fine-tuned model...\n","output_type":"stream"},{"name":"stderr","text":"Calculating perplexity: 100%|██████████| 268/268 [00:26<00:00, 10.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity: 26.33\n","output_type":"stream"},{"name":"stderr","text":"Calculating top-k accuracy: 100%|██████████| 268/268 [05:36<00:00,  1.26s/it]\n","output_type":"stream"},{"name":"stdout","text":"Top-k Accuracies:\n  Top-1: 0.3890\n  Top-5: 0.6075\n  Top-10: 0.6800\nModel saved to ./streamlit_model for Streamlit app\n\nFinal Results Summary:\n         Metric  Value\n     Perplexity  26.33\n Top-1 Accuracy 0.3890\n Top-5 Accuracy 0.6075\nTop-10 Accuracy 0.6800\n\nSample Next Word Predictions:\n==================================================\n\nInput: 'The president of the United States'\nTop 5 predictions:\n  1. ',' (prob: 0.0552)\n  2. ' has' (prob: 0.0548)\n  3. ' is' (prob: 0.0418)\n  4. ' and' (prob: 0.0370)\n  5. ' ,' (prob: 0.0308)\n\nInput: 'Machine learning algorithms can'\nTop 5 predictions:\n  1. ' be' (prob: 0.3429)\n  2. ' also' (prob: 0.0418)\n  3. ' help' (prob: 0.0330)\n  4. ' learn' (prob: 0.0269)\n  5. ' now' (prob: 0.0164)\n\nInput: 'In the field of artificial intelligence'\nTop 5 predictions:\n  1. ' ,' (prob: 0.5697)\n  2. ',' (prob: 0.1071)\n  3. ' (' (prob: 0.0935)\n  4. ' and' (prob: 0.0258)\n  5. ' research' (prob: 0.0156)\n\nInput: 'The weather today is'\nTop 5 predictions:\n  1. ' very' (prob: 0.0423)\n  2. ' a' (prob: 0.0297)\n  3. ' not' (prob: 0.0285)\n  4. ' good' (prob: 0.0244)\n  5. ' generally' (prob: 0.0239)\n\nInput: 'Scientists have discovered'\nTop 5 predictions:\n  1. ' that' (prob: 0.3061)\n  2. ' a' (prob: 0.1613)\n  3. ' the' (prob: 0.0771)\n  4. ' new' (prob: 0.0225)\n  5. ' an' (prob: 0.0211)\n\nTraining complete! Model saved for Streamlit app.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import gradio as gr\nimport torch\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\nimport torch.nn.functional as F\n\n# Load your fine-tuned GPT-2 model\nmodel_path = \"./streamlit_model\"  # your folder\ntokenizer = GPT2Tokenizer.from_pretrained(model_path)\nmodel = GPT2LMHeadModel.from_pretrained(model_path)\nmodel.eval()\n\ndef next_word_prediction(prompt, top_k=5, temperature=1.0):\n    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n    with torch.no_grad():\n        logits = model(input_ids).logits\n        next_token_logits = logits[0, -1, :] / temperature\n        probs = torch.nn.functional.softmax(next_token_logits, dim=-1)\n        top_probs, top_indices = torch.topk(probs, int(top_k))  # ← Fixed this line\n        predictions = []\n        for idx, prob in zip(top_indices, top_probs):\n            token = tokenizer.decode([idx.item()]).strip()\n            if token:\n                predictions.append(f\"{token} ({prob.item()*100:.2f}%)\")\n        return \"\\n\".join(predictions)\n\n\n# Gradio UI\ngr.Interface(\n    fn=next_word_prediction,\n    inputs=[\n        gr.Textbox(label=\"Enter a prompt\"),\n        gr.Slider(1, 10, value=5, label=\"Top-K\"),\n        gr.Slider(0.5, 2.0, value=1.0, label=\"Temperature\")\n    ],\n    outputs=gr.Textbox(label=\"Top Predictions\"),\n    title=\"🧠 Next Word Prediction with Fine-Tuned GPT-2\"\n).launch(share=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T18:59:42.969724Z","iopub.execute_input":"2025-07-09T18:59:42.970455Z","iopub.status.idle":"2025-07-09T18:59:44.614562Z","shell.execute_reply.started":"2025-07-09T18:59:42.970416Z","shell.execute_reply":"2025-07-09T18:59:44.613930Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7861\n* Running on public URL: https://8f5cb32a8354c417dc.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://8f5cb32a8354c417dc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}